{"meta":{"title":"eric_zht","subtitle":"","description":"","author":"eric_zht","url":"http://example.com","root":"/"},"pages":[{"title":"","date":"2025-02-12T00:50:03.386Z","updated":"2025-02-12T00:50:03.386Z","comments":true,"path":"CSS/custom.css","permalink":"http://example.com/CSS/custom.css","excerpt":"","text":"/* 修改标题样式 */ #page-header #site-title, #page-header #site-subtitle{ text-align: left !important; padding-left: 36px; } @media screen and (max-width: 768px) { #page-header #site-title, #page-header #site-subtitle { padding-left: 16px; } } /* 修改头像显示样式 */ .avatar-img img:hover { -webkit-transform: none; -moz-transform: none; -o-transform: none; -ms-transform: none; transform: none; }"}],"posts":[{"title":"Swin-Unet 复现记录（记第一次复现）","slug":"Swin-Unet-复现记录（记第一次复现）","date":"2025-02-28T06:44:00.000Z","updated":"2025-03-01T07:19:30.381Z","comments":true,"path":"2025/02/28/Swin-Unet-复现记录（记第一次复现）/","permalink":"http://example.com/2025/02/28/Swin-Unet-%E5%A4%8D%E7%8E%B0%E8%AE%B0%E5%BD%95%EF%BC%88%E8%AE%B0%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%A4%8D%E7%8E%B0%EF%BC%89/","excerpt":"","text":"一、train中遇到的问题（一）python、pytorch、cuda版本不对应swin-unet官方仓库上写的使用的是python3.7运行的代码，所以我一开始把环境全部朝python3.7去配置。却一直报错。 经过一番搜索后，发现python3.7对应的环境无法在4060laptop上运行。 在多次尝试不同的环境，并结合b站复现别的论文的视频，选择将python版本改为3.8。 1、新建独立环境12conda create -n py.8 python=3.8 # 明确指定Python 3.8conda activate py.8 2、使用pip绕过conda依赖限制1pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118 （二）一堆cuda的报错 根据github中issue的讨论，获得修改方法，train.py中的num_classes和n_class都要设置为9 https://github.com/HuCaoFighting/Swin-Unet/issues/121 （三）安装完requirements.txt中的库后仍然缺少部分库根据搜索安装即可 （四）训练集、验证集地址、名称问题对trainer.py中的相关代码进行如下修改 （五）windows中不能使用多线程二、test中遇到的问题（一）找不到best_model.pth.txt文件 （二）文件地址错乱（一）（二）的解决方法相同： 代码中的volum_path统一改为root_path，然后根据报错提示修改对应的地址。 （三）维度出现问题修改utils.py的代码 原代码： 12image, label = image.squeeze(0).cpu().detach().numpy().squeeze(0), label.squeeze(0).cpu().detach().numpy().squeeze(0) 修改后： 12345678910image = image.cpu().detach().numpy() label = label.cpu().detach().numpy() if image.shape[0] == 1: image = image.squeeze(0) if label.shape[0] == 1: label = label.squeeze(0) #image, label = image.squeeze(0).cpu().detach().numpy().squeeze(0), label.squeeze(0).cpu().detach().numpy().squeeze(0) 参考： https://juejin.cn/post/7431728417744175154 三、test结果第六类不知为啥数值都是0… 四、总结这是我第一次尝试复现代码，用时一天半终于把环境配好，第一次成功运行代码。 用时6:16:35训练完成！！！","categories":[],"tags":[{"name":"Swin-Unet","slug":"Swin-Unet","permalink":"http://example.com/tags/Swin-Unet/"},{"name":"pytorch","slug":"pytorch","permalink":"http://example.com/tags/pytorch/"}]},{"title":"动手学深度学习 2","slug":"动手学深度学习 2","date":"2025-01-24T16:00:00.000Z","updated":"2025-02-17T12:59:54.288Z","comments":true,"path":"2025/01/25/动手学深度学习 2/","permalink":"http://example.com/2025/01/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%202/","excerpt":"","text":"一、卷积卷积有什么用？ 通过卷积核的不同设置，使得每个输出通道可以识别特定的模式，比如识别边缘、锐化、模糊等操作。 核的参数怎么得到的？ 学出来的，不是自己设置的。 卷积尺寸公式： 输出尺寸*=*[输入尺寸-kernel-size+2*padding+stride]/stride 填充在输入周围添加行&#x2F;列，来控制输出形状的减少量 步幅每次滑动kernal窗口时的行&#x2F;列的步长，可以成倍的减少输出形状 注意： 1、第一个公式里的ph是要上下都加了行，所以要乘以二！！！ 2、padding通常设置为k-1 （核-1） 问题1、为什么通常用3x3或者5x5的卷积核呢？他们的视野不是很小吗？（更常用3x3，计算量更小） 多加几层卷积层，最后的到的层会涵盖初始层中很大范围的内容。 多输入通道 这样只能得到单输出的通道 如何得到多输出通道？输入的三通道数据和多个卷积核进行卷积，得到多通道的输出。 co表示卷积核的个数，ci表示卷积核的维度，第0维的卷积层和第0维的输入进行计算，第1维的卷积层和第1维的输入进行计算…，然后将同一位置不同层的计算结果相加，得到这一块的输出内容，再按此方法进行卷积操作得到第一维度的输出。使用其他的卷积核进行相同操作，最后得到多输出通道。 1x1 卷积层 用于不同通道使用不同权重进行融合。 二、最大&#x2F;平均池化返回滑动窗口中的最大值&#x2F;平均值 缓解卷积层对于位置的敏感性，通常放在卷积层之后。 pytorch中，如果不设置默认：池化窗口&#x3D;步幅，就是保证窗口不重叠 为什么现在池化用的少了？ 现在通常用一个卷积层+stride减少输出 三、LeNet 如何检验层的尺寸有没有搭错： 四、AlexNet在LetNet基础上添加了一些层，效果更好 五、VGG块将AlexNet中的多个卷积层封装成一个块，使用多个VGG块构建深度卷积神经网络，效果更好。 不同的卷积块个数和超参数可以得到不同复杂度的变种。 注：在VGG中，内部卷积层的个数n，通道m是超参数。 六、NiN全连接层的问题： 卷积层后的得到第一个全连接层时的计算量会非常大且容易过拟合 NiN块一个卷积层后跟两个全连接层 为什么用的是两个1x1的卷积层？他们其实相当于没有将输入拍扁的全连接层。 NiN架构 总结Nin块使用卷积层加两个1x1卷积层，后者对每个像素增加了非线性性。 Nin用全局平均池化层来替代VGG和AlexNet中的全连接层——不容易过拟合，更少的参数个数。 七、批量归一化 Batch Normalization——加速收敛、网络训练速度 BN层一般用于深层神经网络，浅层的效果不好。 解释由于学习过程中会调整每个层的超参数，当调整前面的层时，会导致后面的层需要重新进行学习，进而导致最后得到的层很难收敛。所以学习率不能设置太高。 批量归一化将每一层的输出进行归一化，使对下一层的输出相似但不完全相同，这样后面的层就不需要改动太大。因此可以选择较大的学习率，加快了网络的训练。 后有论文指出它可能就是通过在每个小批量里加入噪音来控制模型的复杂度。 因此没必要跟丢弃法混合使用。 分布归一化放在非线性激活前面！ 需要训练的参数增加了γ和β，原来的偏置是定好的不需要学习。所以现在有三个参数需要学习。 调包实现nn.BatchNorm2d(输入的通道数) nn.BatchNorm1d(输入的通道数) 八、ResNet 不断添加层数，得到的模型一定最优吗？ 不一定。反而可能会越来越偏离最优函数。 残差块 f(x)&#x3D;x+g(x) 使得新的模型必须包含之前的模型，因此精度不可能变差。 如果g(x)没什么用，那么系统后面给它的梯度会很小，它对最后的结果影响就很小了。 同时，残差块使得很深的网络更加容易训练。 这样加法的操作使得反向传播计算梯度时，即使g(x)的偏导很小，由于是加法，也可以求出x的偏导，那么f(x)得到的梯度就不至于消失。 解决了深层网络底层比较难以训练的问题。——底层拿到的梯度一般比较小。 九、数据增强增加一个已有的数据集，使其有更多的多样性。 ​ 增加不同的背景噪音 ​ 改变图片的颜色和形状 常见增强方法翻转左右、上下翻转 但不是总是可行。比如建筑之类的翻转不太符合实际。但树叶什么的翻转没关系。 切割从图片中切割一块，然后变形到固定形状 随机高宽比（eg.[3&#x2F;4,4&#x2F;3]） 随机大小（eg.[8%,100%]） 随机位置 颜色改变色调，饱和度，明亮度（当前的情况减少50%或增加50%的范围内） 其他https://github.com/aleju/imgaug 十、微调 fine-tune微调中的权重初始化 源数据集远复杂于目标数据，通常微调的效果更好（速度更快、精度越高）。 使用更小的学习率 使用更少的数据迭代 1、重用分类器权重源数据集可能也有目标数据中的部分标号，可以使用预训练好的模型分类器中对应标号对应的向量来做初始化。 2、固定一些层 神经网络通常学习有层次的特征： 低层次的特征更加通用 高层次的特征更加与数据集有关 可以固定底部一些层的参数，不参与更新。 十一、锚框一类目标检测算法是基于锚框的 提出多个被称为锚框的区域 预测每个锚框里是否含有关注的物体 如果是，预测从这个锚框到真实边缘框的偏移 IoU 交并比 赋予锚框标号 第一步的意思就是使用锚框2去预测边缘框3。 一张图有多少个边缘框，就对应有多少个训练样本。 使用非极大值抑制（NMS）输出 每个锚框预测一个边缘框 NMS可以合并相似的预测 选中是非背景类的最大预测值 去掉其他和它IoU值大于θ的预测 重复上述过程知道所有预测要么被选中，要么被去掉 十二、物体检测算法 R-CNN兴趣区域（RoI）池化层 给定一个锚框，均匀分割成n×m块，输出每块里的最大值 不管锚框多大，总是输出nm个值 强行将图像变成大小一样的。 Fast RCNN 使用CNN对图片抽取特征 再使用RoI池化层对每个锚框生成固定长度特征 在原始图片上搜索到锚框后，把锚框按照比例映射到经过CNN层的特征层。 Faster R-CNN Mask R-CNN如果有像素级别的标号，使用FCN来利用这些信息。 总结 十三、单发多框检测 SSD生成锚框 SSD模型 十四、YOLO: you only look once在SSD的基础上进行改进，避免大量SSD重叠。 十五、语义分割语义分割可以识别并理解图像中每一个像素的内容：其语义区域的标注和预测是像素级的。 即每一个像素都有其对应的类别。 列举RGB值和类名12345678910111213#@saveVOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128], [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0], [0, 64, 128]]#@saveVOC_CLASSES = [&#x27;background&#x27;, &#x27;aeroplane&#x27;, &#x27;bicycle&#x27;, &#x27;bird&#x27;, &#x27;boat&#x27;, &#x27;bottle&#x27;, &#x27;bus&#x27;, &#x27;car&#x27;, &#x27;cat&#x27;, &#x27;chair&#x27;, &#x27;cow&#x27;, &#x27;diningtable&#x27;, &#x27;dog&#x27;, &#x27;horse&#x27;, &#x27;motorbike&#x27;, &#x27;person&#x27;, &#x27;potted plant&#x27;, &#x27;sheep&#x27;, &#x27;sofa&#x27;, &#x27;train&#x27;, &#x27;tv/monitor&#x27;] 构建从RGB到VOC类别索引的映射12345678910111213141516171819#@savedef voc_colormap2label(): &quot;&quot;&quot;构建从RGB到VOC类别索引的映射&quot;&quot;&quot; colormap2label = torch.zeros(256 ** 3, dtype=torch.long) for i, colormap in enumerate(VOC_COLORMAP): colormap2label[ (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i # 把RGB三通道的数值当做256进制的数（因为像素最多从0-255）每个像素值算出对应10进制数存入tensor 这样可以对应到每个像素所属类别。 # i表示这个颜色对应类别的序号 return colormap2label#@savedef voc_label_indices(colormap, colormap2label): &quot;&quot;&quot;将VOC标签中的RGB值映射到它们的类别索引&quot;&quot;&quot; colormap = colormap.permute(1, 2, 0).numpy().astype(&#x27;int32&#x27;) # 将输入的彩色标签图像的维度从（通道，高度，宽度）重排为（高度，宽度，通道），然后将其转换为NumPy数组，并将数据类型转换为32位整数，以便进行后续计算。 idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256 + colormap[:, :, 2]) # 计算RGB值对应的类别的索引 return colormap2label[idx] #返回对应类别的序号 十六、转置卷积卷积不会增大输入的高宽，通常要么不变、要么减半。 而转置卷积则可以用来增大输入的高宽。 12345678X = torch.tensor([[0.0, 1.0], [2.0, 3.0]])K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])trans_conv(X, K)X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)#调用api实现转置卷积tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False) #输入通道数，输出通道数，核的大小，是否有偏移tconv.weight.data = Ktconv(X) 填充、步幅和多通道转置卷积的填充（padding）是加在输出上的。 如上图的转置卷积过程，如果padding&#x3D;1，则最后的结果为4。（删除第一和最后的行和列） 对于多个输入和输出通道，转置卷积与常规卷积以相同方式运作。假设输入有$c_i$个通道，且转置卷积为每个输入通道分配了一个$k_h\\times k_w$的卷积核张量。当指定多个输出通道时，每个输出通道将有一个$c_i\\times k_h\\times k_w$的卷积核。 如果我们将$\\mathsf{X}$代入卷积层$f$来输出$\\mathsf{Y}&#x3D;f(\\mathsf{X})$，并创建一个与$f$具有相同的超参数、但输出通道数量是$\\mathsf{X}$中通道数的转置卷积层$g$，那么$g(Y)$的形状将与$\\mathsf{X}$相同。 转置卷积与卷积的转换 十七、全连接卷积神经网络 FCN用转置卷积层来替换CNN最后的全连接层，从而实现每个像素的预测 最后的通道数 &#x3D; 类别数 十八、序列模型时序模型中，当前数据和之间观察到的数据相关。 常见的两种方案 十九、注意力机制卷积、全连接、池化层都只考虑不随意线索。 注意力机制则考虑随意线索 随意线索被称之为查询（query） 每个输入是一个值（value）和不随意线索（key）的对 通过注意力池化层来有偏向性的选择某些输入 注意力机制的本质：https://www.bilibili.com/video/BV1dt4y1J7ov/?share_source=copy_web&amp;spm_id_from=333.788.comment.all.click&amp;vd_source=c675206b339487e9755eec554de241a9 非参的注意力池化层 1、一般情况 2、Nadaraya-Watson 核回归 参数化的注意力机制在之前的基础上引入可以学习的w 注意力分数 加性注意力 additive attention12345678910111213141516171819202122232425#@saveclass AdditiveAttention(nn.Module): &quot;&quot;&quot;加性注意力&quot;&quot;&quot; def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs): super(AdditiveAttention, self).__init__(**kwargs) self.W_k = nn.Linear(key_size, num_hiddens, bias=False) self.W_q = nn.Linear(query_size, num_hiddens, bias=False) self.w_v = nn.Linear(num_hiddens, 1, bias=False) self.dropout = nn.Dropout(dropout) def forward(self, queries, keys, values, valid_lens): queries, keys = self.W_q(queries), self.W_k(keys) # 在维度扩展后， # queries的形状：(batch_size，查询的个数，1，num_hidden) # key的形状：(batch_size，1，“键－值”对的个数，num_hiddens) # 使用广播方式进行求和 —— 广播规则：维度大小为1的轴会自动扩展以匹配另一张量的形状。 # 18行feature的形状：(batch_size，查询的个数，“键－值”对的个数，num_hiddens) features = queries.unsqueeze(2) + keys.unsqueeze(1) features = torch.tanh(features) # self.w_v仅有一个输出，因此从形状中移除最后那个维度。 # scores的形状：(batch_size，查询的个数，“键-值”对的个数) scores = self.w_v(features).squeeze(-1) self.attention_weights = masked_softmax(scores, valid_lens) # 注意力权重 # values的形状：(batch_size，“键－值”对的个数，值的维度) return torch.bmm(self.dropout(self.attention_weights), values) # 将注意力权重与值进行矩阵乘法 为什么要采用广播机制？通过广播机制，一次性生成所有 (query, key) 对的组合特征，避免逐对计算的低效循环。 示例说明假设： 批量大小 batch_size=2 查询数量 num_queries=3 键值对数量 num_kv_pairs=4 隐藏维度 num_hiddens=5 经过 unsqueeze 和广播后： queries 形状：(2, 3, 1, 5) keys 形状：(2, 1, 4, 5) 相加结果 features 形状：(2, 3, 4, 5) 这表示： 对于批量中的每个样本（2个样本）， 每个查询（3个查询）与每个键（4个键）都进行了逐元素相加， 最终得到 3×4=12 个查询-键对的交互特征。 为什么要用masked_softmax？在处理文本数据集时，为了提高计算效率，可能会采用填充的方式使每个文本序列具有相同的长度，便于以相同形状的小批量进行加载，因此可能会存在一些文本序列被填充了没有意义的特殊词源（比如“”词元）。 使用masked_softmax可以过滤掉超出指定范围的位置，不让填充的无意义内容影响结果。 缩放点积注意力见书P290 自注意力完全并行、最长序列为1、但对长序列计算复杂度高 李宏毅：https://www.bilibili.com/video/BV1v3411r78R?spm_id_from=333.788.videopod.episodes&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd&amp;p=2 一、如何得到b1 二、矩阵表示1整体理解 2具体表示 三、位置编码在输入中加入位置信息 二十、编码器-解码器编码器处理输入，解码器生成输出（其实就是把功能集成到一起，然后起了个新名字） 二十一、Transformer 多头注意力 每个注意力池化层都是不同的weight 有掩码的多头注意力解码器对序列中一个元素输出时，不应该考虑该元素之后的元素。 通过掩码实现 计算xi的输出时，假装当前序列长度为i 基于位置的前馈网络 n是序列的长度，不同数据n会变，不能作为特征处理，即n的变化不能影响模型。 层归一化 信息传递 预测 总结 Transformer是一个纯使用注意力的编码-阶码器 编码器和解码器都有n个transformer块 每个块里使用多头（自）注意力、基于位置的前馈网络、 层归一化","categories":[],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://example.com/tags/pytorch/"},{"name":"李沐","slug":"李沐","permalink":"http://example.com/tags/%E6%9D%8E%E6%B2%90/"}]},{"title":"动手学深度学习 1","slug":"动手学深度学习 1","date":"2024-12-02T16:00:00.000Z","updated":"2025-02-11T05:05:36.507Z","comments":true,"path":"2024/12/03/动手学深度学习 1/","permalink":"http://example.com/2024/12/03/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%201/","excerpt":"","text":"一、数据操作+预处理N维数组N维数组是机器学习和神经网络的主要数据结构 0-d 标量：一个数字 1-d 向量：一个特征向量 2-d 矩阵：一个样本-特征矩阵 3-d RGB图片（宽×高×通道） 4-d 一个RGB图片的批量（批量大小batch×宽×高×通道） 创建数组需要： 形状 每个元素的数据类型 每个元素的值 访问元素 左下角子区域：1:3表示[1,3) 第二个子区域： ::3表示行是每3行一跳 ​ ::2表示列是每两列一跳 关于内存x +&#x3D; y 就是直接在原来的x上加上y，与加法的形式不一样 x &#x3D; x + y 本质上是将值给了一个新的x，开辟了个新的内存 矩阵乘法 特征向量：不被矩阵改变方向的向量 矩阵范数的求法： https://www.bilibili.com/video/BV1HD4y1u7yD/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bbecda1ec31c9852a00a62b75b7a6154 矩阵按列求和axis&#x3D;0 行 axis&#x3D;1 列 就是照着那个轴拍扁（全相加） 按单个axis求和比如shape [5,4] axis &#x3D; 1 ,sum:[5] 我不要列了–&gt; 按列向右拍扁 按多个axis求和shape [2,5,4] axis&#x3D;[1,2] sum:[2] 如果 keepdims&#x3D;True 那对应的那个维度就不拍扁shape [2,5,4] axis&#x3D;1， sum：[2,1,4] 二、线性回归是对n维输入的加权，外加偏差$$y&#x3D;w_1x_1+w_2x_2+…+w_nx_n+b$$向量版本：y&#x3D;&lt;w,x&gt;+b 可以看做单层神经网络 衡量预估质量 —— 平方损失 L2 Loss$$l &#x3D; \\frac{1}{2}（y-\\hat{y}）^2$$ 缺点：y‘和y相差很多时梯度太大 -》 使用L1 Loss 参数学习 显示解 基础优化方法梯度下降 选择批量大小 batch_size 小的话可能会引入噪声，但这个噪声可能会使模型预测不走偏，因此准确度可能更高 随机梯度下降（SGD）梯度下降是根据所有样本的平均损失进行计算，需要将所有样本重新计算一遍，非常浪费时间。 因此通常采用小批量随机梯度下降（SGD）进行求解。 三、softmax 回归（多类分类模型）得到每个类的预测置信度 使用交叉熵来衡量预测和真实情况的区别，作为损失函数 回归和分类的区别 无校验比例希望预测出的类的置信度和别的类的置信度的差最大 校验比例 作指数是为了将数值变为非负。并且经过操作后使得加起来和为1 交叉熵损失常用来衡量两个概率的区别 损失函数L1 Loss 缺点：原点处不可导；y’和y离得很近的时候不稳定 -》结合L1和L2 Loss得到下面这个损失函数 Huber’s Robust Loss 鲁棒性 robustness系统的健壮性——系统在特殊情况下的稳定性 四、感知机感知机 只输出一个离散的类，因此只能用于二分类 最早的AI模型之一 求解算法等价于使用批量大小为1的梯度下降 不能拟合XOR函数 多层感知机 MLP（Multilayer Perceptron） 这样得到的结果还是线性的，和单一的线性模型没什么区别（只是加权偏移，限制了对复杂任务的处理能力，只能解决线性问题），因此需要加入非线性激活函数 神经网络为什么working？将同一个输入给不同的神经元，每个神经元学习不同的特性，在最后线性计算合并这些特性，输出结果。 https://www.bilibili.com/video/BV1YD4y1f7p6/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bbecda1ec31c9852a00a62b75b7a6154 非线性激活函数激活函数的本质就是引入非线性性 用relu就好了，用其他的区别也不大。而且relu计算更快 nn.ReLU(inplace=True)中的inplace表示直接原地修改内容，而不是新开内存进行修改，可以节省一点内存。 多隐藏层多隐藏层一般是先扩展再压缩，先压缩的话会损失很多信息。 最后一层不要加非线性激活函数，加了会造成层数的塌陷。 为什么是多层而不是一层很宽？ 一层很宽，即让很多神经元在一起学习，不一定会有很好的效果，可能会导致过拟合。而多层的话相当于一次学一点。—&gt;深度学习 五、模型选择训练误差、泛化误差训练误差：模型在训练数据上的误差 泛化误差：模型在新数据上的误差 验证数据集、测试数据集验证数据集：一个用来评估模型好坏的数据集 例如拿出50%的训练数据 不要跟训练数据混在一起！！ 测试数据集：只用一次的数据集 例如： 未来的考试 房子的实际成交价 K-则交叉验证在没有足够多数据时使用 最后选择最好的那一次的参数作为模型的参数 过拟合、欠拟合 过拟合：数据很简单，模型容量很高，模型可能记住这些数据，但别的数据拟合程度不高。 模型容量拟合各种函数的能力 低容量的模型难以拟合训练数据 高容量的模型可以记住所有的训练数据 首先模型要大，再考虑怎么降低泛化误差 估计模型容量 数据复杂度 样本个数 每个样本的元素个数 时间、空间结构 多样性 六、权重衰退范数高维空间中一点到原点的距离 L1范数 —— 曼哈顿距离各坐标值绝对值相加 L2范数 —— 欧几里得距离高维中的勾股定理 LP范数 正则化 —— 权重衰退每一次会缩小w的取值范围 如果模型很复杂，权重衰退也不会带来很好的效果。 为什么需要正则化？模型可能会过度拟合训练数据，过于依赖训练数据中的噪声和细节。正则化通过降低模型的复杂度来防止过度拟合。 正则化会在损失函数中加入一个正则化项，它会使模型中的某些参数不能太大，因此模型会更倾向于选择那些对预测结果有更大影响的参数，减少对其他参数的依赖。 L1正则化L1正则化可能带来稀疏性，某些特征就不起作用了（去耦合，减少过拟合） L2正则化只缩小了W的权重 如何操作1、手动如果是手动加的话，就是在loss函数中加一个正则化的式子 λ自己试，看看什么时候好 2、在trainer中加参数加一个weight_decay的参数，一般设置成1e-3 这里使用的是L2范数的平方 七、丢弃法 dropout在层之间加噪音 只在训练中使用，在预测中不使用。在测试时，Dropout层仅传递数据 动机一个好的模型需要对输入数据的扰动鲁棒robust 无偏差的加入噪音 一定概率变成0，一定概率x值变大，但期望不变 使用丢弃法通常将丢弃法作用在隐藏全连接层的输出上 全连接层——每一个结点都与上一层的所有结点相连 八、数值稳定性数值爆炸 值超出值域，对于16位浮点数尤为严重 对学习率敏感 ​ 如果学习率太大-&gt;参数值会大-&gt;更大的梯度 ​ 如果学习率太小-&gt;训练无法进展 ​ 需要再训练中不断调整学习率 梯度消失 梯度值变成0 对16位浮点数尤为严重 训练无进展 对底部层尤为严重 仅仅顶部层训练的较好 无法让神经网络更深 让训练更稳定 合理的权重初始值和激活函数的选取可以提升数值稳定性 每一层的输出E&#x3D;0，D&#x3D;一个常数 九、kaggle 房价预测标准化数据预处理标准化数据","categories":[],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://example.com/tags/pytorch/"},{"name":"李沐","slug":"李沐","permalink":"http://example.com/tags/%E6%9D%8E%E6%B2%90/"}]},{"title":"小土堆pytorch 第三天","slug":"小土堆pytorch 第三天","date":"2024-11-26T16:00:00.000Z","updated":"2024-12-02T11:44:15.559Z","comments":true,"path":"2024/11/27/小土堆pytorch 第三天/","permalink":"http://example.com/2024/11/27/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E4%B8%89%E5%A4%A9/","excerpt":"","text":"一、最大池化的使用池化——压缩特征 最大池化——取当前池化核中的最大的数 12345678910111213141516171819202122232425import torchfrom torch import nnfrom torch.nn import MaxPool2dinput = torch.tensor([[1, 2, 0, 3, 1], [0, 1, 2, 3, 1], [1, 2, 1, 0, 0], [5, 2, 3, 1, 1], [2, 1, 0, 1, 1]])input = torch.reshape(input, (-1, 1, 5, 5))class Test(nn.Module): def __init__(self): super(Test,self).__init__() self.maxpool1 = MaxPool2d(kernel_size=3, ceil_mode=True) def forward(self,input): output = self.maxpool1(input) return outputtest = Test()output = test(input)print(output) 二、非线性激活层Relu sigmoid 三、线性层和其他层dropout用于防止过拟合（过于关注噪点之类的，捡了芝麻丢了西瓜） 四、小实战和sequential的使用 sequential作用：类似compose，可以序列化执行操作，使代码更简洁 五、loss1、计算实际输出和目标之间的差距 2、为更新输出提供一定的依据（反向传播，从loss反向修正参数） 这里backward()函数可以反向传播计算梯度（grad），将这个梯度给合适的优化器，可以对神经网络的参数进行更新。 六、优化器 TIPS1、如果某一步的参数不会算，可以先让程序运行到上一步，然后print(output.size)查看对应属性的值 2、要将数据变为浮点数，后面写dtype=torch,float32","categories":[],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://example.com/tags/pytorch/"},{"name":"小土堆","slug":"小土堆","permalink":"http://example.com/tags/%E5%B0%8F%E5%9C%9F%E5%A0%86/"}]},{"title":"小土堆pytorch 第四天","slug":"小土堆pytorch 第四天","date":"2024-11-26T16:00:00.000Z","updated":"2024-12-03T08:42:29.318Z","comments":true,"path":"2024/11/27/小土堆pytorch 第四天/","permalink":"http://example.com/2024/11/27/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E5%9B%9B%E5%A4%A9/","excerpt":"","text":"一、现有模型的加载、修改、添加123456789101112131415import torchvisionfrom torch import nnvgg16_false = torchvision.models.vgg16(pretrained=False)vgg16_true = torchvision.models.vgg16(pretrained=True)print(vgg16_true)train_data = torchvision.datasets.CIFAR10(&quot;dataset&quot;, train=True, transform=torchvision.transforms.ToTensor(),download=True)vgg16_true.classifier.add_module(&#x27;add_linear&#x27;, nn.Linear(1000, 10)) # 添加 层print(vgg16_true)print(vgg16_false)vgg16_false.classifier[6] = nn.Linear(4096, 10) # 修改层的内容print(vgg16_false) 二、网络模型的保存和读取保存123456789import torchimport torchvisionvgg16 = torchvision.models.vgg16()# 保存方式1 模型结构+模型参数torch.save(vgg16, &quot;vgg16_method1.pth&quot;)# 保存方式2 模型参数 （将模型中的参数保存为字典）[官方推荐，存储量小]torch.save(vgg16.state_dict(), &quot;vgg16_method2.pth&quot;) 读取123456789101112131415161718192021222324252627import torchimport torchvisionfrom torch import nn# 方式1 对应保存方式1model = torch.load(&quot;vgg16_method1.pth&quot;)print(model)# 方式2vgg16 = torchvision.models.vgg16()vgg16.load_state_dict(torch.load(&quot;vgg16_method2.pth&quot;))print(vgg16)class Test(nn.Module): def __init__(self): super(Test, self).__init__() self.conv1 = nn.Conv2d(3, 64, 3) def forward(self, x): x = self.conv1(x) return x# 陷阱1 需要引入自己的模型，要么在开头from model_save import *,要么在这个文件中把模型复制过来model = torch.load(&#x27;test_method1.pth&#x27;)print(model) 三、完整模型的训练两种类型分类的模型 模型预测得到的outputs&#x3D;[0.3,0.5] (第一张图) ​ [0.5,0.2] (第二张图) 使用argmax（1）方法可以得到该行最大数的位置，即对应的那个类型 argmax（0）是得到列最大数的位置 这里argmax(1)=[2][1] 将模型预测的结果和真实情况进行比较： 123preds = torch.argmax(1)print((preds == targets).sum) # 得到正确率 yourmodel.train()、yourmodel.eval()对某些特定的层有影响，比如dropout层，详见pytorch官网 四、使用gpu训练法一：改动的地方： 网络模型 数据（输入、检测） 损失函数 改动方法:data = data.cuda() 法二：先在开头定义：device = torch.device(&quot;cuda&quot;) ​ 语法糖写法：device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) 接着像法一一样修改： 12test = Test()test = test.to(device) 五、查看开源项目修改参数时有些参数有required = True,可以把这个参数删掉，然后改成default=&#39;data的地址&#39; TIPSctrl + D 可以复制本行内容","categories":[],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://example.com/tags/pytorch/"},{"name":"小土堆","slug":"小土堆","permalink":"http://example.com/tags/%E5%B0%8F%E5%9C%9F%E5%A0%86/"}]},{"title":"小土堆pytorch 第二天","slug":"小土堆pytorch 第二天","date":"2024-11-25T16:00:00.000Z","updated":"2024-11-27T10:56:58.064Z","comments":true,"path":"2024/11/26/小土堆pytorch 第二天/","permalink":"http://example.com/2024/11/26/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E4%BA%8C%E5%A4%A9/","excerpt":"","text":"一、DataLoader 的使用12345678910111213141516171819import torchvisionfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWritertest_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;, train=False, transform=torchvision.transforms.ToTensor())test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=True) # 每次取的个数 取完后是否打乱 最后如果因为数量无法分配是否舍去writer = SummaryWriter(&quot;dataloader&quot;)for epoch in range(2): step = 0 for data in test_loader: imgs, target = data writer.add_images(f&quot;Epoch:&#123;epoch&#125;&quot;, imgs, step) step = step + 1writer.close() ！！二、python补充： call函数__call__ 可以将类名变为可执行函数 比如在下面的代码中，nn.Module 中包含了__call__函数，使得test(x)直接调用forward函数并得到返回值 call函数举例： 12345678910111213141516class Test(): # def __init__(self): 没内容时可省略不写 def forward(self, input): output = input + 1 return output def __call__(self, input): return self.forward(input) # 在类的方法内部调用另一个方法时，需使用 self 关键字来指向它test = Test()x = 1output = test(x)print(output) 三、神经网络的基本骨架 nn.Module 的使用1234567891011121314151617import torchfrom torch import nnclass Test(nn.Module): # 表示Test继承于Module，它为所有神经网络提供基本的骨架 def __init__(self): super().__init__() def forward(self, input): output = input + 1 return outputtest = Test()x = torch.tensor(1.0)output = test(x)print(output) *四、卷积操作 CONV2D （convolution）（nn.functional.conv2d 为具体方法）12345678910111213141516171819202122import torchimport torch.nn.functional as Finput = torch.tensor([[1, 2, 0, 3, 1], [0, 1, 2, 3, 1], [1, 2, 1, 0, 0], [5, 2, 3, 1, 1], [2, 1, 0, 1, 1]])kernel = torch.tensor([[1, 2, 1], [0, 1, 0], [2, 1, 0]])print(input.shape)input = torch.reshape(input, (1, 1, 5, 5)) # conv2d中要求输入和卷积层的尺寸中有4个属性，而tensor创建出来的只有2个，因此需要reshape增加他们的属性。四个属性分别为：batch_size（每次喂给神经网络多少个数据）填-1的话可以让系统根据后面三个数自动计算，通道数（1为灰度图像，rgb通道数为3），高度，宽度。kernel = torch.reshape(kernel, (1, 1, 3, 3))print(input.shape)output = F.conv2d(input, kernel, stride=1)print(output) Tips: num_workers &gt;0 时可能会出现broken pipe error ，此时把它设置为0试试 举例 padding 举例 stride举例 动图： https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md 五、卷积操作 CONV2D （nn.convv2d 为封装后的函数）（实际使用）123456789101112131415161718192021222324252627import torchimport torchvisionfrom torch import nnfrom torch.nn import Conv2dfrom torch.utils.data import DataLoaderdataset = torchvision.datasets.CIFAR10(&quot;dataset&quot;, train=False, transform=torchvision.transforms.ToTensor(),download=True)DataLoader = DataLoader(dataset, batch_size=64)class Test(nn.Module): def __init__(self): super(Test, self).__init__() self.conv1 = Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0) def forward(self, x): x=self.conv1(x) return xtest = Test()for data in DataLoader: imgs, targets = data output = test(imgs) print(imgs.shape) print(output.shape) in_channels=3: imput的通道为3 out_channels=6：output的通道为6 &#x3D;卷积核的个数 kernel_size=3：卷积核的高和宽为3 深度由系统自动计算 stride=1：步径为1 padding=0：边缘不需要加行&#x2F;列 注：卷积核中的数值应是自动生成的 训练模型就是训练卷积核中自动生成的数值 *找到的资源对知名模型的代码进行逐行解读的网站 https://nn.labml.ai/ 图神经网络的实现基本都有 Deep Graph Library https://www.dgl.ai","categories":[],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://example.com/tags/pytorch/"},{"name":"小土堆","slug":"小土堆","permalink":"http://example.com/tags/%E5%B0%8F%E5%9C%9F%E5%A0%86/"}]},{"title":"小土堆pytorch 第一天","slug":"小土堆pytorch 第一天","date":"2024-11-23T16:00:00.000Z","updated":"2024-11-24T14:57:18.659Z","comments":true,"path":"2024/11/24/小土堆pytorch 第一天/","permalink":"http://example.com/2024/11/24/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E4%B8%80%E5%A4%A9/","excerpt":"","text":"一、两大法宝函数1、dir()打开，看见 2、help()查看说明书 二、三个运行方式的区别 三、如何导入数据两种数据形式：Dataset、Dataloader Dataset1、如何获取每一个数据及其label？ 2、总共有多少条数据？ 12345678910111213141516171819202122232425262728293031from torch.utils.data import Datasetfrom PIL import Imageimport os # 用于获取图片的地址class MyData(Dataset): def __init__(self, root_dir, label_dir): self.root_dir = root_dir self.label_dir = label_dir self.path = str(os.path.join(self.root_dir, self.label_dir)) # 拼接地址 self.img_path = os.listdir(self.path) # 获取当前目录中文件的地址列表 def __getitem__(self, idx): img_name = self.img_path[idx] # 获取文件名 img_item_path = os.path.join(self.root_dir,self.label_dir,img_name) # 拼接地址 img = Image.open(img_item_path) label = self.label_dir return img, label def __len__(self): return len(self.img_path)root_dir = &quot;dataset/train&quot;ants_label_dir = &quot;ants&quot;bees_label_dir = &quot;bees&quot;ants_dataset = MyData(root_dir, ants_label_dir)bees_dataset = MyData(root_dir, bees_label_dir)train_dataset = ants_dataset + bees_dataset 四、Tensorboard的使用12345678from torch.utils.tensorboard import SummaryWriterwriter = SummaryWriter(&quot;logs&quot;) # 文件存放在logs文件夹中for i in range(100): writer.add_scalar(&quot;y=2x&quot;, 2*i, i) # tag 纵轴 横轴writer.close() 如何打开tensorboard界面？终端中： tensorboard –logdir&#x3D;”D:\\pycharm\\learn_pytorch\\learn_pytorch\\logs” 如何切换打开的端口（避免服务器训练时与别人冲突）tensorboard –logdir&#x3D;”D:\\pycharm\\learn_pytorch\\learn_pytorch\\logs” –port&#x3D;6007（修改端口地址） 导入自己的图片123456789101112from torch.utils.tensorboard import SummaryWriterimport numpy as npfrom PIL import Imagewriter = SummaryWriter(&quot;logs&quot;)image_path = &quot;dataset/train/ants/0013035.jpg&quot;img_PIL = Image.open(image_path)img_array = np.array(img_PIL) # 将图片转为numpy格式writer.add_image(&quot;test&quot;, img_array, 1, dataformats=&#x27;HWC&#x27;)# 从PIL到numpy，需要在add_image()中指定shape中每一个数字、维度表示的含义writer.close() 如果改变image的地址并且将writer.add_image(&quot;test&quot;, img_array, 1（横轴）, dataformats=&#39;HWC&#39;)中的横轴改为2，则tensorboard会在之前的图片上显示拖动条，向右拖动即为第二张图片 五、transform的使用 1234567891011121314151617181920212223242526272829303132333435363738394041424344from PIL import Imagefrom torch.utils.tensorboard import SummaryWriterfrom torchvision import transformswriter = SummaryWriter(&quot;logs&quot;)img = Image.open(&quot;dataset/train/ants/0013035.jpg&quot;)print(img)# ToTensor (transforms的一种工具)trans_totensor = transforms.ToTensor() # 打开并配置工具img_totensor = trans_totensor(img) # 使用工具writer.add_image(&quot;ToTensor&quot;, img_totensor)# Normalize 归一化 (预处理，确保输入模型的数据在相同的尺度上。将数据缩放到一个小范围)trans_norm = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])img_norm = trans_norm(img_totensor)writer.add_image(&quot;Normalize&quot;, img_norm)# Resizeprint(img.size)trans_resize = transforms.Resize((512, 512))img_resize = trans_resize(img) # 得到的仍然是PIL类型img_resize = trans_totensor(img_resize) # 得到tensor类型的图片writer.add_image(&quot;Resize&quot;, img_resize, 0)print(img_resize)# 方法2 Compose - resize 相当于打包处理的过程trans_resize_2 = transforms.Resize(512)trans_compose = transforms.Compose([trans_resize_2, trans_totensor])# 将resize和转换为tensor的操作用compose打包,第一个操作的输出是第二个操作的输入img_resize2 = trans_compose(img)writer.add_image(&quot;Resize&quot;, img_resize2, 1)# RandomCrop 随机裁剪trans_randomCrop = transforms.RandomCrop((500, 1000)) # 只写一个数就是按正方形裁剪trans_compose2 = transforms.Compose([trans_randomCrop, trans_totensor])for i in range(10): img_crop = trans_compose2(img) writer.add_image(&quot;RandomCropHW&quot;, img_crop, i)writer.close() transforms.Normalize(mean, std)注：需要传递tensor类型图片 将图像的每个通道（RGB，共3个通道）按特定的均值和标准差进行归一化 Resize()注：需要传递PIL类型图片 括号中只给一个数值，那么就将图片短的那个边匹配这个数值进行等比缩放 括号中给两个数值，就将长宽设置为这两个数值 Tips关注输入和输出类型 关注方法需要什么参数 不知道返回值的时候，可以print()或print(type())或debug","categories":[],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://example.com/tags/pytorch/"},{"name":"小土堆","slug":"小土堆","permalink":"http://example.com/tags/%E5%B0%8F%E5%9C%9F%E5%A0%86/"}]}],"categories":[],"tags":[{"name":"Swin-Unet","slug":"Swin-Unet","permalink":"http://example.com/tags/Swin-Unet/"},{"name":"pytorch","slug":"pytorch","permalink":"http://example.com/tags/pytorch/"},{"name":"李沐","slug":"李沐","permalink":"http://example.com/tags/%E6%9D%8E%E6%B2%90/"},{"name":"小土堆","slug":"小土堆","permalink":"http://example.com/tags/%E5%B0%8F%E5%9C%9F%E5%A0%86/"}]}