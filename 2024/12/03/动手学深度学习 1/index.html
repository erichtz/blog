<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>动手学深度学习 1 | eric_zht</title><meta name="author" content="eric_zht"><meta name="copyright" content="eric_zht"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一、数据操作+预处理N维数组N维数组是机器学习和神经网络的主要数据结构 0-d 标量：一个数字 1-d 向量：一个特征向量 2-d 矩阵：一个样本-特征矩阵 3-d RGB图片（宽×高×通道） 4-d 一个RGB图片的批量（批量大小batch×宽×高×通道） 创建数组需要：  形状  每个元素的数据类型  每个元素的值   访问元素 左下角子区域：1:3表示[1,3) 第二个子区域： ::3表示行">
<meta property="og:type" content="article">
<meta property="og:title" content="动手学深度学习 1">
<meta property="og:url" content="http://example.com/2024/12/03/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%201/index.html">
<meta property="og:site_name" content="eric_zht">
<meta property="og:description" content="一、数据操作+预处理N维数组N维数组是机器学习和神经网络的主要数据结构 0-d 标量：一个数字 1-d 向量：一个特征向量 2-d 矩阵：一个样本-特征矩阵 3-d RGB图片（宽×高×通道） 4-d 一个RGB图片的批量（批量大小batch×宽×高×通道） 创建数组需要：  形状  每个元素的数据类型  每个元素的值   访问元素 左下角子区域：1:3表示[1,3) 第二个子区域： ::3表示行">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/image/icon.png">
<meta property="article:published_time" content="2024-12-02T16:00:00.000Z">
<meta property="article:modified_time" content="2025-02-11T05:05:36.507Z">
<meta property="article:author" content="eric_zht">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="李沐">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/image/icon.png"><link rel="shortcut icon" href="/image/favicon.png"><link rel="canonical" href="http://example.com/2024/12/03/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%201/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '动手学深度学习 1',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/image/icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">eric_zht</span></a><a class="nav-page-title" href="/"><span class="site-name">动手学深度学习 1</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">动手学深度学习 1</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-12-02T16:00:00.000Z" title="发表于 2024-12-03 00:00:00">2024-12-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-02-11T05:05:36.507Z" title="更新于 2025-02-11 13:05:36">2025-02-11</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">2.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>6分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="一、数据操作-预处理"><a href="#一、数据操作-预处理" class="headerlink" title="一、数据操作+预处理"></a>一、数据操作+预处理</h1><h2 id="N维数组"><a href="#N维数组" class="headerlink" title="N维数组"></a>N维数组</h2><p>N维数组是机器学习和神经网络的主要数据结构</p>
<p>0-d 标量：一个数字</p>
<p>1-d 向量：一个特征向量</p>
<p>2-d 矩阵：一个样本-特征矩阵</p>
<p>3-d RGB图片（宽×高×通道）</p>
<p>4-d 一个RGB图片的批量（批量大小batch×宽×高×通道）</p>
<h2 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a>创建数组</h2><p>需要：</p>
<ol>
<li><p>形状</p>
</li>
<li><p>每个元素的数据类型</p>
</li>
<li><p>每个元素的值</p>
</li>
</ol>
<h2 id="访问元素"><a href="#访问元素" class="headerlink" title="访问元素"></a>访问元素</h2><p><img src="https://pic.ericzht.space/PicGo/image-20241203172021813.png" alt="image-20241203172021813"></p>
<p>左下角子区域：1:3表示[1,3)</p>
<p>第二个子区域： ::3表示行是每3行一跳</p>
<p>​                           ::2表示列是每两列一跳</p>
<h2 id="关于内存"><a href="#关于内存" class="headerlink" title="关于内存"></a>关于内存</h2><p>x +&#x3D; y   就是直接在原来的x上加上y，与加法的形式不一样</p>
<p>x &#x3D; x + y  本质上是将值给了一个新的x，开辟了个新的内存</p>
<h2 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h2><p><img src="https://pic.ericzht.space/PicGo/image-20241204093735764.png" alt="image-20241204093735764"></p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241204093821205.png" alt="image-20241204093821205"></p>
<p>特征向量：不被矩阵改变方向的向量</p>
<p>矩阵范数的求法：</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1HD4y1u7yD/?spm_id_from=333.337.search-card.all.click&vd_source=bbecda1ec31c9852a00a62b75b7a6154">https://www.bilibili.com/video/BV1HD4y1u7yD/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bbecda1ec31c9852a00a62b75b7a6154</a></p>
<h2 id="矩阵按列求和"><a href="#矩阵按列求和" class="headerlink" title="矩阵按列求和"></a>矩阵按列求和</h2><p>axis&#x3D;0 行</p>
<p>axis&#x3D;1 列</p>
<p>就是照着那个轴拍扁（全相加）</p>
<h3 id="按单个axis求和"><a href="#按单个axis求和" class="headerlink" title="按单个axis求和"></a>按单个axis求和</h3><p>比如shape [5,4]</p>
<p>axis &#x3D; 1 ,sum:[5]  我不要列了–&gt; 按列向右拍扁</p>
<h3 id="按多个axis求和"><a href="#按多个axis求和" class="headerlink" title="按多个axis求和"></a>按多个axis求和</h3><p>shape [2,5,4]</p>
<p>axis&#x3D;[1,2]  sum:[2]</p>
<h3 id="如果-keepdims-True-那对应的那个维度就不拍扁"><a href="#如果-keepdims-True-那对应的那个维度就不拍扁" class="headerlink" title="如果 keepdims&#x3D;True 那对应的那个维度就不拍扁"></a>如果 keepdims&#x3D;True 那对应的那个维度就不拍扁</h3><p>shape [2,5,4]</p>
<p>axis&#x3D;1， sum：[2,1,4]</p>
<h1 id="二、线性回归"><a href="#二、线性回归" class="headerlink" title="二、线性回归"></a>二、线性回归</h1><p>是对n维输入的加权，外加偏差<br>$$<br>y&#x3D;w_1x_1+w_2x_2+…+w_nx_n+b<br>$$<br>向量版本：y&#x3D;&lt;w,x&gt;+b</p>
<p>可以看做单层神经网络</p>
<h2 id="衡量预估质量-——-平方损失-L2-Loss"><a href="#衡量预估质量-——-平方损失-L2-Loss" class="headerlink" title="衡量预估质量 —— 平方损失 L2 Loss"></a>衡量预估质量 —— 平方损失 L2 Loss</h2><p>$$<br>l &#x3D; \frac{1}{2}（y-\hat{y}）^2<br>$$</p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241204185859597.png" alt="image-20241204185859597"></p>
<p><strong>缺点：</strong>y‘和y相差很多时梯度太大  -》 使用L1 Loss</p>
<h2 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h2><p><img src="https://pic.ericzht.space/PicGo/image-20241204154124850.png" alt="image-20241204154124850"></p>
<h2 id="显示解"><a href="#显示解" class="headerlink" title="显示解"></a>显示解</h2><p><img src="https://pic.ericzht.space/PicGo/image-20241204154306164.png" alt="image-20241204154306164"></p>
<h2 id="基础优化方法"><a href="#基础优化方法" class="headerlink" title="基础优化方法"></a>基础优化方法</h2><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p><img src="https://pic.ericzht.space/PicGo/image-20241204155610564.png" alt="image-20241204155610564"></p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241204155650413.png" alt="image-20241204155650413"></p>
<h3 id="选择批量大小"><a href="#选择批量大小" class="headerlink" title="选择批量大小"></a>选择批量大小</h3><p><img src="https://pic.ericzht.space/PicGo/image-20241204160000557.png" alt="image-20241204160000557"></p>
<p>batch_size 小的话可能会引入噪声，但这个噪声可能会使模型预测不走偏，因此准确度可能更高</p>
<h3 id="随机梯度下降（SGD）"><a href="#随机梯度下降（SGD）" class="headerlink" title="随机梯度下降（SGD）"></a>随机梯度下降（SGD）</h3><p>梯度下降是根据所有样本的平均损失进行计算，需要将所有样本重新计算一遍，非常浪费时间。</p>
<p><strong>因此通常采用小批量随机梯度下降（SGD）进行求解。</strong></p>
<h1 id="三、softmax-回归（多类分类模型）"><a href="#三、softmax-回归（多类分类模型）" class="headerlink" title="三、softmax 回归（多类分类模型）"></a>三、softmax 回归（多类分类模型）</h1><p>得到每个类的预测置信度</p>
<p>使用交叉熵来衡量预测和真实情况的区别，作为损失函数</p>
<h2 id="回归和分类的区别"><a href="#回归和分类的区别" class="headerlink" title="回归和分类的区别"></a>回归和分类的区别</h2><p><img src="https://pic.ericzht.space/PicGo/image-20241204165608832.png" alt="image-20241204165608832"></p>
<h2 id="无校验比例"><a href="#无校验比例" class="headerlink" title="无校验比例"></a>无校验比例</h2><p>希望<em><strong>预测出的类</strong></em>的置信度和<em><strong>别的类</strong></em>的置信度的差最大</p>
<h2 id="校验比例"><a href="#校验比例" class="headerlink" title="校验比例"></a>校验比例</h2><p><img src="https://pic.ericzht.space/PicGo/image-20241204170751785.png" alt="image-20241204170751785"></p>
<p>作指数是为了将数值变为非负。并且经过操作后使得加起来和为1</p>
<h2 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h2><p>常用来衡量两个概率的区别</p>
<img src="https://pic.ericzht.space/PicGo/image-20241204171019482.png" alt="image-20241204171019482" style="zoom:50%;" />

<img src="https://pic.ericzht.space/PicGo/image-20241204171116885.png" alt="image-20241204171116885" style="zoom: 50%;" />

<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="L1-Loss"><a href="#L1-Loss" class="headerlink" title="L1 Loss"></a>L1 Loss</h3><p><img src="https://pic.ericzht.space/PicGo/image-20241204190106547.png" alt="image-20241204190106547"></p>
<p><strong>缺点：</strong>原点处不可导；y’和y离得很近的时候不稳定 -》结合L1和L2 Loss得到下面这个损失函数</p>
<h3 id="Huber’s-Robust-Loss"><a href="#Huber’s-Robust-Loss" class="headerlink" title="Huber’s Robust Loss"></a>Huber’s Robust Loss</h3><p><img src="https://pic.ericzht.space/PicGo/image-20241204190202725.png" alt="image-20241204190202725"></p>
<h2 id="鲁棒性-robustness"><a href="#鲁棒性-robustness" class="headerlink" title="鲁棒性 robustness"></a>鲁棒性 robustness</h2><p>系统的健壮性——系统在特殊情况下的稳定性</p>
<h1 id="四、感知机"><a href="#四、感知机" class="headerlink" title="四、感知机"></a>四、感知机</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p><img src="https://pic.ericzht.space/PicGo/image-20241204202800593.png" alt="image-20241204202800593"></p>
<p>只输出一个离散的类，因此只能用于<strong>二分类</strong></p>
<ol>
<li>最早的AI模型之一</li>
<li>求解算法等价于使用批量大小为1的梯度下降</li>
<li>不能拟合XOR函数</li>
</ol>
<h2 id="多层感知机-MLP（Multilayer-Perceptron）"><a href="#多层感知机-MLP（Multilayer-Perceptron）" class="headerlink" title="多层感知机 MLP（Multilayer Perceptron）"></a>多层感知机 <em>MLP</em>（Multilayer Perceptron）</h2><p><img src="https://pic.ericzht.space/PicGo/image-20241204205122365.png" alt="image-20241204205122365"></p>
<p>这样得到的结果还是线性的，和单一的线性模型没什么区别（只是加权偏移，限制了对复杂任务的处理能力，只能解决线性问题），因此需要加入<strong>非线性激活函数</strong></p>
<h3 id="神经网络为什么working？"><a href="#神经网络为什么working？" class="headerlink" title="神经网络为什么working？"></a>神经网络为什么working？</h3><p>将同一个输入给不同的神经元，每个神经元学习不同的特性，在最后线性计算合并这些特性，输出结果。</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1YD4y1f7p6/?spm_id_from=333.337.search-card.all.click&vd_source=bbecda1ec31c9852a00a62b75b7a6154">https://www.bilibili.com/video/BV1YD4y1f7p6/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bbecda1ec31c9852a00a62b75b7a6154</a></p>
<h3 id="非线性激活函数"><a href="#非线性激活函数" class="headerlink" title="非线性激活函数"></a>非线性激活函数</h3><p><em><strong>激活函数的本质就是引入非线性性</strong></em></p>
<p>用relu就好了，用其他的区别也不大。而且relu计算更快</p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241204205224974.png" alt="image-20241204205224974"></p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241204205325193.png" alt="image-20241204205325193"></p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241204205411680.png" alt="image-20241204205411680"></p>
<p><code>nn.ReLU(inplace=True)</code>中的inplace表示直接原地修改内容，而不是新开内存进行修改，可以节省一点内存。</p>
<h3 id="多隐藏层"><a href="#多隐藏层" class="headerlink" title="多隐藏层"></a>多隐藏层</h3><p>多隐藏层一般是先扩展再压缩，先压缩的话会损失很多信息。</p>
<p>最后一层不要加非线性激活函数，加了会造成层数的塌陷。</p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241204210708019.png" alt="image-20241204210708019"></p>
<p><strong>为什么是多层而不是一层很宽？</strong></p>
<p>一层很宽，即让很多神经元在一起学习，不一定会有很好的效果，可能会导致过拟合。而多层的话相当于一次学一点。—&gt;深度学习</p>
<h1 id="五、模型选择"><a href="#五、模型选择" class="headerlink" title="五、模型选择"></a>五、模型选择</h1><h2 id="训练误差、泛化误差"><a href="#训练误差、泛化误差" class="headerlink" title="训练误差、泛化误差"></a>训练误差、泛化误差</h2><p>训练误差：模型在训练数据上的误差</p>
<p>泛化误差：模型在新数据上的误差</p>
<h2 id="验证数据集、测试数据集"><a href="#验证数据集、测试数据集" class="headerlink" title="验证数据集、测试数据集"></a>验证数据集、测试数据集</h2><p>验证数据集：一个用来评估模型好坏的数据集</p>
<ul>
<li>例如拿出50%的训练数据</li>
<li>不要跟训练数据混在一起！！</li>
</ul>
<p>测试数据集：只用一次的数据集</p>
<p>例如：</p>
<ul>
<li>未来的考试</li>
<li>房子的实际成交价</li>
</ul>
<h2 id="K-则交叉验证"><a href="#K-则交叉验证" class="headerlink" title="K-则交叉验证"></a>K-则交叉验证</h2><p>在没有足够多数据时使用</p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241205203809267.png" alt="image-20241205203809267"></p>
<p><strong>最后选择最好的那一次的参数作为模型的参数</strong></p>
<h2 id="过拟合、欠拟合"><a href="#过拟合、欠拟合" class="headerlink" title="过拟合、欠拟合"></a>过拟合、欠拟合</h2><img src="https://pic.ericzht.space/PicGo/image-20241205204242125.png" alt="image-20241205204242125" style="zoom:50%;" />

<img src="https://pic.ericzht.space/PicGo/image-20241205204358790.png" alt="image-20241205204358790" style="zoom:50%;" />

<p>过拟合：数据很简单，模型容量很高，模型可能记住这些数据，但别的数据拟合程度不高。</p>
<h2 id="模型容量"><a href="#模型容量" class="headerlink" title="模型容量"></a>模型容量</h2><p>拟合各种函数的能力</p>
<ol>
<li>低容量的模型难以拟合训练数据</li>
<li>高容量的模型可以记住所有的训练数据</li>
</ol>
<img src="https://pic.ericzht.space/PicGo/image-20241205204630093.png" alt="image-20241205204630093" style="zoom:50%;" />

<p><strong>首先模型要大，再考虑怎么降低泛化误差</strong></p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241205205100162.png" alt="image-20241205205100162"></p>
<h3 id="估计模型容量"><a href="#估计模型容量" class="headerlink" title="估计模型容量"></a>估计模型容量</h3><p><img src="https://pic.ericzht.space/PicGo/image-20241205205301358.png" alt="image-20241205205301358"></p>
<h2 id="数据复杂度"><a href="#数据复杂度" class="headerlink" title="数据复杂度"></a>数据复杂度</h2><ul>
<li>样本个数</li>
<li>每个样本的元素个数</li>
<li>时间、空间结构</li>
<li>多样性</li>
</ul>
<h1 id="六、权重衰退"><a href="#六、权重衰退" class="headerlink" title="六、权重衰退"></a>六、权重衰退</h1><h2 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h2><p>高维空间中一点到原点的距离</p>
<h3 id="L1范数-——-曼哈顿距离"><a href="#L1范数-——-曼哈顿距离" class="headerlink" title="L1范数 —— 曼哈顿距离"></a>L1范数 —— 曼哈顿距离</h3><p>各坐标值绝对值相加</p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241207155850492.png" alt="image-20241207155850492"></p>
<h3 id="L2范数-——-欧几里得距离"><a href="#L2范数-——-欧几里得距离" class="headerlink" title="L2范数 —— 欧几里得距离"></a>L2范数 —— 欧几里得距离</h3><p>高维中的勾股定理</p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241207155811269.png" alt="image-20241207155811269"></p>
<h3 id="LP范数"><a href="#LP范数" class="headerlink" title="LP范数"></a>LP范数</h3><p><img src="https://pic.ericzht.space/PicGo/image-20241207155945232.png" alt="image-20241207155945232"></p>
<h2 id="正则化-——-权重衰退"><a href="#正则化-——-权重衰退" class="headerlink" title="正则化 —— 权重衰退"></a>正则化 —— 权重衰退</h2><p>每一次会缩小w的取值范围</p>
<p><strong>如果模型很复杂，权重衰退也不会带来很好的效果。</strong></p>
<h3 id="为什么需要正则化？"><a href="#为什么需要正则化？" class="headerlink" title="为什么需要正则化？"></a>为什么需要正则化？</h3><p>模型可能会过度拟合训练数据，过于依赖训练数据中的噪声和细节。正则化通过降低模型的复杂度来防止过度拟合。</p>
<p>正则化会在损失函数中加入一个正则化项，它会使模型中的某些参数不能太大，因此模型会更倾向于选择那些对预测结果有更大影响的参数，减少对其他参数的依赖。</p>
<img src="https://pic.ericzht.space/PicGo/image-20241207162432013.png" alt="image-20241207162432013" style="zoom:67%;" />

<h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>L1正则化可能带来稀疏性，某些特征就不起作用了（去耦合，减少过拟合）</p>
<h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>只缩小了W的权重</p>
<h3 id="如何操作"><a href="#如何操作" class="headerlink" title="如何操作"></a>如何操作</h3><h4 id="1、手动"><a href="#1、手动" class="headerlink" title="1、手动"></a>1、手动</h4><p>如果是手动加的话，就是在loss函数中加一个正则化的式子</p>
<p><strong>λ自己试，看看什么时候好</strong></p>
<h4 id="2、在trainer中加参数"><a href="#2、在trainer中加参数" class="headerlink" title="2、在trainer中加参数"></a>2、在trainer中加参数</h4><p>加一个weight_decay的参数，一般设置成1e-3</p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241207184655244.png" alt="image-20241207184655244"></p>
<p>这里使用的是L2范数的平方</p>
<h1 id="七、丢弃法-dropout"><a href="#七、丢弃法-dropout" class="headerlink" title="七、丢弃法 dropout"></a>七、丢弃法 dropout</h1><p>在层之间加噪音</p>
<p>只在训练中使用，在预测中不使用。在测试时，<code>Dropout</code>层仅传递数据</p>
<h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>一个好的模型需要对输入数据的扰动鲁棒robust</p>
<h2 id="无偏差的加入噪音"><a href="#无偏差的加入噪音" class="headerlink" title="无偏差的加入噪音"></a>无偏差的加入噪音</h2><img src="https://pic.ericzht.space/PicGo/image-20241207193214310.png" alt="image-20241207193214310" style="zoom: 33%;" />

<p>一定概率变成0，一定概率x值变大，但期望不变</p>
<h2 id="使用丢弃法"><a href="#使用丢弃法" class="headerlink" title="使用丢弃法"></a>使用丢弃法</h2><p>通常将丢弃法作用在<strong>隐藏全连接层</strong>的输出上</p>
<p>全连接层——每一个结点都与上一层的所有结点相连</p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241207193520066.png" alt="image-20241207193520066"></p>
<h1 id="八、数值稳定性"><a href="#八、数值稳定性" class="headerlink" title="八、数值稳定性"></a>八、数值稳定性</h1><h2 id="数值爆炸"><a href="#数值爆炸" class="headerlink" title="数值爆炸"></a>数值爆炸</h2><ol>
<li><p>值超出值域，对于16位浮点数尤为严重</p>
</li>
<li><p>对学习率敏感</p>
<ul>
<li><p>​	如果学习率太大-&gt;参数值会大-&gt;更大的梯度</p>
</li>
<li><p>​	如果学习率太小-&gt;训练无法进展</p>
</li>
<li><p>​	需要再训练中不断调整学习率</p>
</li>
</ul>
</li>
</ol>
<h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><ol>
<li>梯度值变成0<ul>
<li>对16位浮点数尤为严重</li>
</ul>
</li>
<li>训练无进展</li>
<li>对底部层尤为严重<ul>
<li>仅仅顶部层训练的较好</li>
<li>无法让神经网络更深</li>
</ul>
</li>
</ol>
<h2 id="让训练更稳定"><a href="#让训练更稳定" class="headerlink" title="让训练更稳定"></a>让训练更稳定</h2><img src="https://pic.ericzht.space/PicGo/image-20241207205634808.png" alt="image-20241207205634808" style="zoom:50%;" />

<p>合理的权重初始值和激活函数的选取可以提升数值稳定性</p>
<p>每一层的输出E&#x3D;0，D&#x3D;一个常数</p>
<h1 id="九、kaggle-房价预测"><a href="#九、kaggle-房价预测" class="headerlink" title="九、kaggle 房价预测"></a>九、kaggle 房价预测</h1><h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h2><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p><strong>标准化数据</strong></p>
<p><img src="https://pic.ericzht.space/PicGo/image-20241208160943334.png" alt="image-20241208160943334"></p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a><a class="post-meta__tags" href="/tags/%E6%9D%8E%E6%B2%90/">李沐</a></div><div class="post-share"><div class="social-share" data-image="/image/icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/01/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%202/" title="动手学深度学习 2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">动手学深度学习 2</div></div><div class="info-2"><div class="info-item-1">一、卷积卷积有什么用？ 通过卷积核的不同设置，使得每个输出通道可以识别特定的模式，比如识别边缘、锐化、模糊等操作。 核的参数怎么得到的？ 学出来的，不是自己设置的。 卷积尺寸公式： 输出尺寸*=*[输入尺寸-kernel-size+2*padding+stride]/stride 填充在输入周围添加行&#x2F;列，来控制输出形状的减少量 步幅每次滑动kernal窗口时的行&#x2F;列的步长，可以成倍的减少输出形状  注意： 1、第一个公式里的ph是要上下都加了行，所以要乘以二！！！ 2、padding通常设置为k-1...</div></div></div></a><a class="pagination-related" href="/2024/11/27/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E4%B8%89%E5%A4%A9/" title="小土堆pytorch 第三天"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">小土堆pytorch 第三天</div></div><div class="info-2"><div class="info-item-1">一、最大池化的使用池化——压缩特征 最大池化——取当前池化核中的最大的数  12345678910111213141516171819202122232425import torchfrom torch import nnfrom torch.nn import MaxPool2dinput = torch.tensor([[1, 2, 0, 3, 1],                      [0, 1, 2, 3, 1],                      [1, 2, 1, 0, 0],                      [5, 2, 3, 1, 1],                      [2, 1, 0, 1, 1]])input = torch.reshape(input, (-1, 1, 5, 5))class Test(nn.Module):    def __init__(self):        super(Test,self).__init__()        self.maxpool1 =...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/01/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%202/" title="动手学深度学习 2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-25</div><div class="info-item-2">动手学深度学习 2</div></div><div class="info-2"><div class="info-item-1">一、卷积卷积有什么用？ 通过卷积核的不同设置，使得每个输出通道可以识别特定的模式，比如识别边缘、锐化、模糊等操作。 核的参数怎么得到的？ 学出来的，不是自己设置的。 卷积尺寸公式： 输出尺寸*=*[输入尺寸-kernel-size+2*padding+stride]/stride 填充在输入周围添加行&#x2F;列，来控制输出形状的减少量 步幅每次滑动kernal窗口时的行&#x2F;列的步长，可以成倍的减少输出形状  注意： 1、第一个公式里的ph是要上下都加了行，所以要乘以二！！！ 2、padding通常设置为k-1...</div></div></div></a><a class="pagination-related" href="/2025/02/28/Swin-Unet-%E5%A4%8D%E7%8E%B0%E8%AE%B0%E5%BD%95%EF%BC%88%E8%AE%B0%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%A4%8D%E7%8E%B0%EF%BC%89/" title="Swin-Unet 复现记录（记第一次复现）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-28</div><div class="info-item-2">Swin-Unet 复现记录（记第一次复现）</div></div><div class="info-2"><div class="info-item-1">一、train中遇到的问题（一）python、pytorch、cuda版本不对应swin-unet官方仓库上写的使用的是python3.7运行的代码，所以我一开始把环境全部朝python3.7去配置。却一直报错。 经过一番搜索后，发现python3.7对应的环境无法在4060laptop上运行。 在多次尝试不同的环境，并结合b站复现别的论文的视频，选择将python版本改为3.8。 1、新建独立环境12conda create -n py.8 python=3.8  # 明确指定Python 3.8conda activate py.8  2、使用pip绕过conda依赖限制1pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url...</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E4%B8%80%E5%A4%A9/" title="小土堆pytorch 第一天"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">小土堆pytorch 第一天</div></div><div class="info-2"><div class="info-item-1">一、两大法宝函数1、dir()打开，看见 2、help()查看说明书 二、三个运行方式的区别 三、如何导入数据两种数据形式：Dataset、Dataloader Dataset1、如何获取每一个数据及其label？ 2、总共有多少条数据？ 12345678910111213141516171819202122232425262728293031from torch.utils.data import Datasetfrom PIL import Imageimport os  # 用于获取图片的地址class MyData(Dataset):    def __init__(self, root_dir, label_dir):        self.root_dir = root_dir        self.label_dir = label_dir        self.path = str(os.path.join(self.root_dir, self.label_dir))   # 拼接地址        self.img_path =...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/image/icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">eric_zht</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">一、数据操作+预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#N%E7%BB%B4%E6%95%B0%E7%BB%84"><span class="toc-text">N维数组</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%95%B0%E7%BB%84"><span class="toc-text">创建数组</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE%E5%85%83%E7%B4%A0"><span class="toc-text">访问元素</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E5%86%85%E5%AD%98"><span class="toc-text">关于内存</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-text">矩阵乘法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C"><span class="toc-text">矩阵按列求和</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%89%E5%8D%95%E4%B8%AAaxis%E6%B1%82%E5%92%8C"><span class="toc-text">按单个axis求和</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%89%E5%A4%9A%E4%B8%AAaxis%E6%B1%82%E5%92%8C"><span class="toc-text">按多个axis求和</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C-keepdims-True-%E9%82%A3%E5%AF%B9%E5%BA%94%E7%9A%84%E9%82%A3%E4%B8%AA%E7%BB%B4%E5%BA%A6%E5%B0%B1%E4%B8%8D%E6%8B%8D%E6%89%81"><span class="toc-text">如果 keepdims&#x3D;True 那对应的那个维度就不拍扁</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">二、线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%A1%E9%87%8F%E9%A2%84%E4%BC%B0%E8%B4%A8%E9%87%8F-%E2%80%94%E2%80%94-%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1-L2-Loss"><span class="toc-text">衡量预估质量 —— 平方损失 L2 Loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="toc-text">参数学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%98%BE%E7%A4%BA%E8%A7%A3"><span class="toc-text">显示解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-text">基础优化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%89%E6%8B%A9%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F"><span class="toc-text">选择批量大小</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88SGD%EF%BC%89"><span class="toc-text">随机梯度下降（SGD）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81softmax-%E5%9B%9E%E5%BD%92%EF%BC%88%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%EF%BC%89"><span class="toc-text">三、softmax 回归（多类分类模型）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E5%92%8C%E5%88%86%E7%B1%BB%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">回归和分类的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E6%A0%A1%E9%AA%8C%E6%AF%94%E4%BE%8B"><span class="toc-text">无校验比例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%A1%E9%AA%8C%E6%AF%94%E4%BE%8B"><span class="toc-text">校验比例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1"><span class="toc-text">交叉熵损失</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#L1-Loss"><span class="toc-text">L1 Loss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Huber%E2%80%99s-Robust-Loss"><span class="toc-text">Huber’s Robust Loss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%B2%81%E6%A3%92%E6%80%A7-robustness"><span class="toc-text">鲁棒性 robustness</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text">四、感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-text">感知机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-MLP%EF%BC%88Multilayer-Perceptron%EF%BC%89"><span class="toc-text">多层感知机 MLP（Multilayer Perceptron）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BB%80%E4%B9%88working%EF%BC%9F"><span class="toc-text">神经网络为什么working？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-text">非线性激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%9A%90%E8%97%8F%E5%B1%82"><span class="toc-text">多隐藏层</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-text">五、模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E3%80%81%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-text">训练误差、泛化误差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%81%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">验证数据集、测试数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K-%E5%88%99%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-text">K-则交叉验证</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-text">过拟合、欠拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F"><span class="toc-text">模型容量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%B0%E8%AE%A1%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F"><span class="toc-text">估计模型容量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-text">数据复杂度</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80"><span class="toc-text">六、权重衰退</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%8C%83%E6%95%B0"><span class="toc-text">范数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#L1%E8%8C%83%E6%95%B0-%E2%80%94%E2%80%94-%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB"><span class="toc-text">L1范数 —— 曼哈顿距离</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L2%E8%8C%83%E6%95%B0-%E2%80%94%E2%80%94-%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB"><span class="toc-text">L2范数 —— 欧几里得距离</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LP%E8%8C%83%E6%95%B0"><span class="toc-text">LP范数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96-%E2%80%94%E2%80%94-%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80"><span class="toc-text">正则化 —— 权重衰退</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%9F"><span class="toc-text">为什么需要正则化？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L1%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">L1正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L2%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">L2正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%93%8D%E4%BD%9C"><span class="toc-text">如何操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E6%89%8B%E5%8A%A8"><span class="toc-text">1、手动</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E5%9C%A8trainer%E4%B8%AD%E5%8A%A0%E5%8F%82%E6%95%B0"><span class="toc-text">2、在trainer中加参数</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E4%B8%A2%E5%BC%83%E6%B3%95-dropout"><span class="toc-text">七、丢弃法 dropout</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E6%9C%BA"><span class="toc-text">动机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E5%81%8F%E5%B7%AE%E7%9A%84%E5%8A%A0%E5%85%A5%E5%99%AA%E9%9F%B3"><span class="toc-text">无偏差的加入噪音</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="toc-text">使用丢弃法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7"><span class="toc-text">八、数值稳定性</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E7%88%86%E7%82%B8"><span class="toc-text">数值爆炸</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="toc-text">梯度消失</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A9%E8%AE%AD%E7%BB%83%E6%9B%B4%E7%A8%B3%E5%AE%9A"><span class="toc-text">让训练更稳定</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B9%9D%E3%80%81kaggle-%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B"><span class="toc-text">九、kaggle 房价预测</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-text">标准化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">数据预处理</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/28/Swin-Unet-%E5%A4%8D%E7%8E%B0%E8%AE%B0%E5%BD%95%EF%BC%88%E8%AE%B0%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%A4%8D%E7%8E%B0%EF%BC%89/" title="Swin-Unet 复现记录（记第一次复现）">Swin-Unet 复现记录（记第一次复现）</a><time datetime="2025-02-28T06:44:00.000Z" title="发表于 2025-02-28 14:44:00">2025-02-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%202/" title="动手学深度学习 2">动手学深度学习 2</a><time datetime="2025-01-24T16:00:00.000Z" title="发表于 2025-01-25 00:00:00">2025-01-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/03/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%201/" title="动手学深度学习 1">动手学深度学习 1</a><time datetime="2024-12-02T16:00:00.000Z" title="发表于 2024-12-03 00:00:00">2024-12-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/27/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E4%B8%89%E5%A4%A9/" title="小土堆pytorch 第三天">小土堆pytorch 第三天</a><time datetime="2024-11-26T16:00:00.000Z" title="发表于 2024-11-27 00:00:00">2024-11-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/27/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E5%9B%9B%E5%A4%A9/" title="小土堆pytorch 第四天">小土堆pytorch 第四天</a><time datetime="2024-11-26T16:00:00.000Z" title="发表于 2024-11-27 00:00:00">2024-11-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By eric_zht</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>