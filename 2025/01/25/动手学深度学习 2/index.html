<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>动手学深度学习 2 | eric_zht</title><meta name="author" content="eric_zht"><meta name="copyright" content="eric_zht"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一、卷积卷积有什么用？ 通过卷积核的不同设置，使得每个输出通道可以识别特定的模式，比如识别边缘、锐化、模糊等操作。 核的参数怎么得到的？ 学出来的，不是自己设置的。 卷积尺寸公式： 输出尺寸*&#x3D;*[输入尺寸-kernel-size+2*padding+stride]&#x2F;stride 填充在输入周围添加行&#x2F;列，来控制输出形状的减少量 步幅每次滑动kernal窗口时的行&#x2F;列的步长，可">
<meta property="og:type" content="article">
<meta property="og:title" content="动手学深度学习 2">
<meta property="og:url" content="http://example.com/2025/01/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%202/index.html">
<meta property="og:site_name" content="eric_zht">
<meta property="og:description" content="一、卷积卷积有什么用？ 通过卷积核的不同设置，使得每个输出通道可以识别特定的模式，比如识别边缘、锐化、模糊等操作。 核的参数怎么得到的？ 学出来的，不是自己设置的。 卷积尺寸公式： 输出尺寸*&#x3D;*[输入尺寸-kernel-size+2*padding+stride]&#x2F;stride 填充在输入周围添加行&#x2F;列，来控制输出形状的减少量 步幅每次滑动kernal窗口时的行&#x2F;列的步长，可">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/image/icon.png">
<meta property="article:published_time" content="2025-01-24T16:00:00.000Z">
<meta property="article:modified_time" content="2025-02-10T13:22:33.334Z">
<meta property="article:author" content="eric_zht">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="李沐">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/image/icon.png"><link rel="shortcut icon" href="/image/favicon.png"><link rel="canonical" href="http://example.com/2025/01/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%202/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '动手学深度学习 2',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/image/icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">eric_zht</span></a><a class="nav-page-title" href="/"><span class="site-name">动手学深度学习 2</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">动手学深度学习 2</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-01-24T16:00:00.000Z" title="发表于 2025-01-25 00:00:00">2025-01-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-02-10T13:22:33.334Z" title="更新于 2025-02-10 21:22:33">2025-02-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">2.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="一、卷积"><a href="#一、卷积" class="headerlink" title="一、卷积"></a>一、卷积</h1><p><em><strong>卷积有什么用？</strong></em></p>
<p>通过卷积核的不同设置，使得每个输出通道可以识别特定的模式，比如识别边缘、锐化、模糊等操作。</p>
<p><em><strong>核的参数怎么得到的？</strong></em></p>
<p>学出来的，不是自己设置的。</p>
<p><em><strong>卷积尺寸公式：</strong></em></p>
<p><code>输出尺寸*=*[输入尺寸-kernel-size+2*padding+stride]/stride</code></p>
<h2 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h2><p>在输入周围添加行&#x2F;列，来控制输出形状的减少量</p>
<h2 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h2><p>每次滑动kernal窗口时的行&#x2F;列的步长，可以成倍的减少输出形状</p>
<p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250125203559067.png" alt="image-20250125203559067"></p>
<p><strong>注意：</strong></p>
<p><strong>1、第一个公式里的ph是要上下都加了行，所以要乘以二！！！</strong></p>
<p><strong>2、padding通常设置为k-1 （核-1）</strong></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>1、为什么通常用3x3或者5x5的卷积核呢？他们的视野不是很小吗？（更常用3x3，计算量更小）</p>
<p>多加几层卷积层，最后的到的层会涵盖初始层中很大范围的内容。</p>
<img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250125210159175.png" alt="image-20250125210159175"  />



<h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250126204241191.png" alt="image-20250126204241191"></p>
<p>这样只能得到单输出的通道</p>
<h3 id="如何得到多输出通道？"><a href="#如何得到多输出通道？" class="headerlink" title="如何得到多输出通道？"></a><strong>如何得到多输出通道？</strong></h3><p>输入的三通道数据和多个卷积核进行卷积，得到多通道的输出。</p>
<p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250126204931218.png" alt="image-20250126204931218"></p>
<p>co表示卷积核的个数，ci表示卷积核的维度，第0维的卷积层和第0维的输入进行计算，第1维的卷积层和第1维的输入进行计算…，然后将同一位置不同层的计算结果相加，得到这一块的输出内容，再按此方法进行卷积操作得到第一维度的输出。使用<strong>其他的卷积核</strong>进行相同操作，最后得到多输出通道。</p>
<h2 id="1x1-卷积层"><a href="#1x1-卷积层" class="headerlink" title="1x1 卷积层"></a>1x1 卷积层</h2><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250126210139909.png" alt="image-20250126210139909"></p>
<p>用于不同通道使用不同权重进行融合。</p>
<h1 id="二、最大-平均池化"><a href="#二、最大-平均池化" class="headerlink" title="二、最大&#x2F;平均池化"></a>二、最大&#x2F;平均池化</h1><p>返回滑动窗口中的最大值&#x2F;平均值</p>
<p>缓解卷积层对于位置的敏感性，通常放在卷积层之后。</p>
<p><strong>pytorch中，如果不设置默认：池化窗口&#x3D;步幅，就是保证窗口不重叠</strong></p>
<p>为什么现在池化用的少了？</p>
<p>现在通常用一个卷积层+stride减少输出</p>
<h1 id="三、LeNet"><a href="#三、LeNet" class="headerlink" title="三、LeNet"></a>三、LeNet</h1><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250202195341765.png" alt="image-20250202195341765"></p>
<h2 id="如何检验层的尺寸有没有搭错："><a href="#如何检验层的尺寸有没有搭错：" class="headerlink" title="如何检验层的尺寸有没有搭错："></a>如何检验层的尺寸有没有搭错：</h2><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250202200046443.png" alt="image-20250202200046443"></p>
<h1 id="四、AlexNet"><a href="#四、AlexNet" class="headerlink" title="四、AlexNet"></a>四、AlexNet</h1><p>在LetNet基础上添加了一些层，效果更好</p>
<h1 id="五、VGG块"><a href="#五、VGG块" class="headerlink" title="五、VGG块"></a>五、VGG块</h1><p>将AlexNet中的多个卷积层封装成一个块，使用多个VGG块构建深度卷积神经网络，效果更好。</p>
<p>不同的卷积块个数和超参数可以得到不同复杂度的变种。</p>
<p>注：在VGG中，内部卷积层的个数n，通道m是超参数。</p>
<h1 id="六、NiN"><a href="#六、NiN" class="headerlink" title="六、NiN"></a>六、NiN</h1><p>全连接层的问题：</p>
<p>卷积层后的得到第一个全连接层时的<strong>计算量会非常大</strong>且<strong>容易过拟合</strong></p>
<p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250203204835160.png" alt="image-20250203204835160"></p>
<h2 id="NiN块"><a href="#NiN块" class="headerlink" title="NiN块"></a>NiN块</h2><p>一个卷积层后跟两个全连接层</p>
<p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250203204956898.png" alt="image-20250203204956898"></p>
<p>为什么用的是两个1x1的卷积层？他们其实相当于<strong>没有将输入拍扁的全连接层</strong>。</p>
<h2 id="NiN架构"><a href="#NiN架构" class="headerlink" title="NiN架构"></a>NiN架构</h2><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250203205310762.png" alt="image-20250203205310762"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Nin块使用卷积层加两个1x1卷积层，后者对每个像素增加了非线性性。</p>
<p>Nin用全局平均池化层来替代VGG和AlexNet中的全连接层——不容易过拟合，更少的参数个数。</p>
<h1 id="七、批量归一化-Batch-Normalization"><a href="#七、批量归一化-Batch-Normalization" class="headerlink" title="七、批量归一化 Batch Normalization"></a>七、批量归一化 Batch Normalization</h1><h1 id="——加速收敛、网络训练速度"><a href="#——加速收敛、网络训练速度" class="headerlink" title="——加速收敛、网络训练速度"></a>——加速收敛、网络训练速度</h1><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250204164534195.png" alt="image-20250204164534195"></p>
<p><strong>BN层一般用于深层神经网络，浅层的效果不好。</strong></p>
<h2 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h2><p>由于学习过程中会调整每个层的超参数，当调整前面的层时，会导致后面的层需要重新进行学习，进而导致最后得到的层很难收敛。所以学习率不能设置太高。</p>
<p>批量归一化将每一层的输出进行归一化，使对下一层的输出相似但不完全相同，这样后面的层就不需要改动太大。因此可以选择较大的学习率，加快了网络的训练。 </p>
<p><em><strong>后有论文指出它可能就是通过在每个小批量里加入噪音来控制模型的复杂度。</strong></em></p>
<p><strong>因此没必要跟丢弃法混合使用。</strong></p>
<p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250204165011101.png" alt="image-20250204165011101"></p>
<p><strong>分布归一化放在非线性激活前面！</strong></p>
<p>需要训练的参数增加了γ和β，原来的偏置是定好的不需要学习。所以现在有三个参数需要学习。</p>
<h2 id="调包实现"><a href="#调包实现" class="headerlink" title="调包实现"></a>调包实现</h2><p>nn.BatchNorm2d(输入的通道数)</p>
<p>nn.BatchNorm1d(输入的通道数)</p>
<h1 id="八、ResNet"><a href="#八、ResNet" class="headerlink" title="八、ResNet"></a>八、ResNet</h1><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250204200225258.png" alt="image-20250204200225258"></p>
<p><em><strong>不断添加层数，得到的模型一定最优吗？</strong></em></p>
<p>不一定。反而可能会越来越偏离最优函数。</p>
<h2 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h2><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250204200355148.png" alt="image-20250204200355148"></p>
<p>f(x)&#x3D;x+g(x)</p>
<p>使得新的模型必须包含之前的模型，因此精度不可能变差。</p>
<ul>
<li>如果g(x)没什么用，那么系统后面给它的梯度会很小，它对最后的结果影响就很小了。</li>
</ul>
<p><strong>同时，残差块使得很深的网络更加容易训练。</strong></p>
<p>这样加法的操作使得反向传播计算梯度时，即使g(x)的偏导很小，由于是加法，也可以求出x的偏导，那么f(x)得到的梯度就不至于消失。</p>
<p><em>解决了深层网络底层比较难以训练的问题。</em>——底层拿到的梯度一般比较小。</p>
<h1 id="九、数据增强"><a href="#九、数据增强" class="headerlink" title="九、数据增强"></a>九、数据增强</h1><p>增加一个已有的数据集，使其有更多的多样性。</p>
<ul>
<li>​	增加不同的背景噪音</li>
<li>​	改变图片的颜色和形状</li>
</ul>
<h2 id="常见增强方法"><a href="#常见增强方法" class="headerlink" title="常见增强方法"></a>常见增强方法</h2><h3 id="翻转"><a href="#翻转" class="headerlink" title="翻转"></a>翻转</h3><p>左右、上下翻转</p>
<p>但不是总是可行。比如建筑之类的翻转不太符合实际。但树叶什么的翻转没关系。</p>
<h3 id="切割"><a href="#切割" class="headerlink" title="切割"></a>切割</h3><p>从图片中切割一块，然后变形到固定形状</p>
<ul>
<li>随机高宽比（eg.[3&#x2F;4,4&#x2F;3]）</li>
<li>随机大小（eg.[8%,100%]）</li>
<li>随机位置</li>
</ul>
<h3 id="颜色"><a href="#颜色" class="headerlink" title="颜色"></a>颜色</h3><p>改变色调，饱和度，明亮度（当前的情况减少50%或增加50%的范围内）</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p><a target="_blank" rel="noopener" href="https://github.com/aleju/imgaug">https://github.com/aleju/imgaug</a></p>
<h1 id="十、微调"><a href="#十、微调" class="headerlink" title="十、微调"></a>十、微调</h1><h2 id="微调中的权重初始化"><a href="#微调中的权重初始化" class="headerlink" title="微调中的权重初始化"></a>微调中的权重初始化</h2><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250206203248814.png" alt="image-20250206203248814"></p>
<p>源数据集远复杂于目标数据，通常微调的效果更好（速度更快、精度越高）。</p>
<ul>
<li><p>使用更小的学习率</p>
</li>
<li><p>使用更少的数据迭代</p>
</li>
</ul>
<h2 id="1、重用分类器权重"><a href="#1、重用分类器权重" class="headerlink" title="1、重用分类器权重"></a>1、重用分类器权重</h2><p>源数据集可能也有目标数据中的部分标号，可以使用预训练好的模型分类器中对应标号对应的向量来做初始化。</p>
<h2 id="2、固定一些层"><a href="#2、固定一些层" class="headerlink" title="2、固定一些层"></a>2、固定一些层</h2><ul>
<li>神经网络通常学习有层次的特征：<ul>
<li>低层次的特征更加通用</li>
<li>高层次的特征更加与数据集有关</li>
</ul>
</li>
<li>可以固定底部一些层的参数，不参与更新。</li>
</ul>
<h1 id="十一、锚框"><a href="#十一、锚框" class="headerlink" title="十一、锚框"></a>十一、锚框</h1><p>一类目标检测算法是基于锚框的</p>
<ul>
<li>提出多个被称为锚框的区域</li>
<li>预测每个锚框里是否含有关注的物体</li>
<li>如果是，预测从这个锚框到真实边缘框的偏移</li>
</ul>
<h2 id="IoU-交并比"><a href="#IoU-交并比" class="headerlink" title="IoU 交并比"></a>IoU 交并比</h2><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250208160454237.png" alt="image-20250208160454237" style="zoom:67%;" />

<img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250208160523457.png" alt="image-20250208160523457" style="zoom:67%;" />

<h2 id="赋予锚框标号"><a href="#赋予锚框标号" class="headerlink" title="赋予锚框标号"></a>赋予锚框标号</h2><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250209105249553.png" alt="image-20250209105249553"></p>
<p>第一步的意思就是使用锚框2去预测边缘框3。</p>
<p><strong>一张图有多少个边缘框，就对应有多少个训练样本。</strong></p>
<h2 id="使用非极大值抑制（NMS）输出"><a href="#使用非极大值抑制（NMS）输出" class="headerlink" title="使用非极大值抑制（NMS）输出"></a>使用非极大值抑制（NMS）输出</h2><ul>
<li>每个锚框预测一个边缘框</li>
<li>NMS可以合并相似的预测<ul>
<li>选中是非背景类的最大预测值</li>
<li>去掉其他和它IoU值大于θ的预测</li>
<li>重复上述过程知道所有预测要么被选中，要么被去掉</li>
</ul>
</li>
</ul>
<img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250209105958709.png" alt="image-20250209105958709" style="zoom: 50%;" />

<h1 id="十二、物体检测算法-R-CNN"><a href="#十二、物体检测算法-R-CNN" class="headerlink" title="十二、物体检测算法 R-CNN"></a>十二、物体检测算法 R-CNN</h1><h2 id="兴趣区域（RoI）池化层"><a href="#兴趣区域（RoI）池化层" class="headerlink" title="兴趣区域（RoI）池化层"></a>兴趣区域（RoI）池化层</h2><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250209163934488.png" alt="image-20250209163934488"></p>
<p>给定一个锚框，均匀分割成n×m块，输出每块里的最大值</p>
<p>不管锚框多大，总是输出nm个值</p>
<p><strong>强行将图像变成大小一样的。</strong></p>
<h2 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h2><ul>
<li>使用CNN对图片抽取特征</li>
<li>再使用RoI池化层对每个锚框生成固定长度特征</li>
</ul>
<p><strong>在原始图片上搜索到锚框后，把锚框按照比例映射到经过CNN层的特征层。</strong></p>
<img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250209165216146.png" alt="image-20250209165216146" style="zoom:50%;" />

<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250209165440613.png" alt="image-20250209165440613"></p>
<h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><p>如果有像素级别的标号，使用FCN来利用这些信息。</p>
<p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250209170559072.png" alt="image-20250209170559072"></p>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250209171622411.png" alt="image-20250209171622411"></p>
<h1 id="十三、单发多框检测-SSD"><a href="#十三、单发多框检测-SSD" class="headerlink" title="十三、单发多框检测 SSD"></a>十三、单发多框检测 SSD</h1><h2 id="生成锚框"><a href="#生成锚框" class="headerlink" title="生成锚框"></a>生成锚框</h2><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250209172045212.png" alt="image-20250209172045212"></p>
<h2 id="SSD模型"><a href="#SSD模型" class="headerlink" title="SSD模型"></a>SSD模型</h2><p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250209172314880.png" alt="image-20250209172314880"></p>
<h1 id="十四、YOLO-you-only-look-once"><a href="#十四、YOLO-you-only-look-once" class="headerlink" title="十四、YOLO: you only look once"></a>十四、YOLO: you only look once</h1><p><strong>在SSD的基础上进行改进，避免大量SSD重叠。</strong></p>
<p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250209173141837.png" alt="image-20250209173141837"></p>
<h1 id="十五、语义分割"><a href="#十五、语义分割" class="headerlink" title="十五、语义分割"></a>十五、语义分割</h1><p>语义分割可以识别并理解图像中每一个像素的内容：其语义区域的标注和预测是像素级的。</p>
<p>即每一个像素都有其对应的类别。</p>
<h2 id="列举RGB值和类名"><a href="#列举RGB值和类名" class="headerlink" title="列举RGB值和类名"></a>列举RGB值和类名</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line">VOC_COLORMAP = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">192</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">192</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">128</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line">VOC_CLASSES = [<span class="string">&#x27;background&#x27;</span>, <span class="string">&#x27;aeroplane&#x27;</span>, <span class="string">&#x27;bicycle&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;boat&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;bottle&#x27;</span>, <span class="string">&#x27;bus&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;chair&#x27;</span>, <span class="string">&#x27;cow&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;diningtable&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;motorbike&#x27;</span>, <span class="string">&#x27;person&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;potted plant&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;sofa&#x27;</span>, <span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;tv/monitor&#x27;</span>]</span><br></pre></td></tr></table></figure>



<h2 id="构建从RGB到VOC类别索引的映射"><a href="#构建从RGB到VOC类别索引的映射" class="headerlink" title="构建从RGB到VOC类别索引的映射"></a>构建从RGB到VOC类别索引的映射</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">voc_colormap2label</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;构建从RGB到VOC类别索引的映射&quot;&quot;&quot;</span></span><br><span class="line">    colormap2label = torch.zeros(<span class="number">256</span> ** <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">    <span class="keyword">for</span> i, colormap <span class="keyword">in</span> <span class="built_in">enumerate</span>(VOC_COLORMAP):</span><br><span class="line">        colormap2label[</span><br><span class="line">            (colormap[<span class="number">0</span>] * <span class="number">256</span> + colormap[<span class="number">1</span>]) * <span class="number">256</span> + colormap[<span class="number">2</span>]] = i</span><br><span class="line">        <span class="comment"># 把RGB三通道的数值当做256进制的数（因为像素最多从0-255）每个像素值算出对应10进制数存入tensor 这样可以对应到每个像素所属类别。</span></span><br><span class="line">        <span class="comment"># i表示这个颜色对应类别的序号</span></span><br><span class="line">    <span class="keyword">return</span> colormap2label</span><br><span class="line"></span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">voc_label_indices</span>(<span class="params">colormap, colormap2label</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将VOC标签中的RGB值映射到它们的类别索引&quot;&quot;&quot;</span></span><br><span class="line">    colormap = colormap.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).numpy().astype(<span class="string">&#x27;int32&#x27;</span>) </span><br><span class="line">    <span class="comment"># 将输入的彩色标签图像的维度从（通道，高度，宽度）重排为（高度，宽度，通道），然后将其转换为NumPy数组，并将数据类型转换为32位整数，以便进行后续计算。</span></span><br><span class="line">    idx = ((colormap[:, :, <span class="number">0</span>] * <span class="number">256</span> + colormap[:, :, <span class="number">1</span>]) * <span class="number">256</span></span><br><span class="line">           + colormap[:, :, <span class="number">2</span>]) <span class="comment"># 计算RGB值对应的类别的索引</span></span><br><span class="line">    <span class="keyword">return</span> colormap2label[idx] <span class="comment">#返回对应类别的序号</span></span><br></pre></td></tr></table></figure>



<h1 id="十六、转置卷积"><a href="#十六、转置卷积" class="headerlink" title="十六、转置卷积"></a>十六、转置卷积</h1><p>卷积不会增大输入的高宽，通常要么不变、要么减半。</p>
<p>而转置卷积则可以用来增大输入的高宽。</p>
<p><img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250210202817711.png" alt="image-20250210202817711"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">trans_conv(X, K)</span><br><span class="line">X, K = X.reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), K.reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment">#调用api实现转置卷积</span></span><br><span class="line">tconv = nn.ConvTranspose2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, bias=<span class="literal">False</span>) <span class="comment">#输入通道数，输出通道数，核的大小，是否有偏移</span></span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure>

<h2 id="填充、步幅和多通道"><a href="#填充、步幅和多通道" class="headerlink" title="填充、步幅和多通道"></a>填充、步幅和多通道</h2><p>转置卷积的填充（padding）是加在<strong>输出</strong>上的。</p>
<p>如上图的转置卷积过程，如果padding&#x3D;1，则最后的结果为4。（删除第一和最后的行和列）</p>
<img src="C:\Users\19355\AppData\Roaming\Typora\typora-user-images\image-20250210211025587.png" alt="image-20250210211025587"  />

<p><strong>对于多个输入和输出通道，转置卷积与常规卷积以相同方式运作。</strong><br>假设输入有$c_i$个通道，且转置卷积为每个输入通道分配了一个$k_h\times k_w$的卷积核张量。<br>当指定多个输出通道时，每个输出通道将有一个$c_i\times k_h\times k_w$的卷积核。</p>
<p>如果我们将$\mathsf{X}$代入卷积层$f$来输出$\mathsf{Y}&#x3D;f(\mathsf{X})$，并创建一个与$f$具有相同的超参数、但输出通道数量是$\mathsf{X}$中通道数的转置卷积层$g$，那么$g(Y)$的形状将与$\mathsf{X}$相同。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a><a class="post-meta__tags" href="/tags/%E6%9D%8E%E6%B2%90/">李沐</a></div><div class="post-share"><div class="social-share" data-image="/image/icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2024/12/03/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%201/" title="动手学深度学习 1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">动手学深度学习 1</div></div><div class="info-2"><div class="info-item-1">一、数据操作+预处理N维数组N维数组是机器学习和神经网络的主要数据结构 0-d 标量：一个数字 1-d 向量：一个特征向量 2-d 矩阵：一个样本-特征矩阵 3-d RGB图片（宽×高×通道） 4-d 一个RGB图片的批量（批量大小batch×宽×高×通道） 创建数组需要：  形状  每个元素的数据类型  每个元素的值   访问元素 左下角子区域：1:3表示[1,3) 第二个子区域： ::3表示行是每3行一跳 ​                           ::2表示列是每两列一跳 关于内存x +&#x3D; y   就是直接在原来的x上加上y，与加法的形式不一样 x &#x3D; x + y ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/12/03/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%201/" title="动手学深度学习 1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-03</div><div class="info-item-2">动手学深度学习 1</div></div><div class="info-2"><div class="info-item-1">一、数据操作+预处理N维数组N维数组是机器学习和神经网络的主要数据结构 0-d 标量：一个数字 1-d 向量：一个特征向量 2-d 矩阵：一个样本-特征矩阵 3-d RGB图片（宽×高×通道） 4-d 一个RGB图片的批量（批量大小batch×宽×高×通道） 创建数组需要：  形状  每个元素的数据类型  每个元素的值   访问元素 左下角子区域：1:3表示[1,3) 第二个子区域： ::3表示行是每3行一跳 ​                           ::2表示列是每两列一跳 关于内存x +&#x3D; y   就是直接在原来的x上加上y，与加法的形式不一样 x &#x3D; x + y ...</div></div></div></a><a class="pagination-related" href="/2024/11/24/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E4%B8%80%E5%A4%A9/" title="小土堆pytorch 第一天"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-24</div><div class="info-item-2">小土堆pytorch 第一天</div></div><div class="info-2"><div class="info-item-1">一、两大法宝函数1、dir()打开，看见 2、help()查看说明书 二、三个运行方式的区别 三、如何导入数据两种数据形式：Dataset、Dataloader Dataset1、如何获取每一个数据及其label？ 2、总共有多少条数据？ 12345678910111213141516171819202122232425262728293031from torch.utils.data import Datasetfrom PIL import Imageimport os  # 用于获取图片的地址class MyData(Dataset):    def __init__(self, root_dir, label_dir):        self.root_dir = root_dir        self.label_dir = label_dir        self.path = str(os.path.join(self.root_dir, self.label_dir))   # 拼接地址        self.img_path =...</div></div></div></a><a class="pagination-related" href="/2024/11/26/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E4%BA%8C%E5%A4%A9/" title="小土堆pytorch 第二天"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-26</div><div class="info-item-2">小土堆pytorch 第二天</div></div><div class="info-2"><div class="info-item-1">一、DataLoader 的使用12345678910111213141516171819import torchvisionfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWritertest_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;, train=False, transform=torchvision.transforms.ToTensor())test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=True)                                        # 每次取的个数      取完后是否打乱            最后如果因为数量无法分配是否舍去writer =...</div></div></div></a><a class="pagination-related" href="/2024/11/27/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E4%B8%89%E5%A4%A9/" title="小土堆pytorch 第三天"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-27</div><div class="info-item-2">小土堆pytorch 第三天</div></div><div class="info-2"><div class="info-item-1">一、最大池化的使用池化——压缩特征 最大池化——取当前池化核中的最大的数  12345678910111213141516171819202122232425import torchfrom torch import nnfrom torch.nn import MaxPool2dinput = torch.tensor([[1, 2, 0, 3, 1],                      [0, 1, 2, 3, 1],                      [1, 2, 1, 0, 0],                      [5, 2, 3, 1, 1],                      [2, 1, 0, 1, 1]])input = torch.reshape(input, (-1, 1, 5, 5))class Test(nn.Module):    def __init__(self):        super(Test,self).__init__()        self.maxpool1 =...</div></div></div></a><a class="pagination-related" href="/2024/11/27/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E5%9B%9B%E5%A4%A9/" title="小土堆pytorch 第四天"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-27</div><div class="info-item-2">小土堆pytorch 第四天</div></div><div class="info-2"><div class="info-item-1">一、现有模型的加载、修改、添加123456789101112131415import torchvisionfrom torch import nnvgg16_false = torchvision.models.vgg16(pretrained=False)vgg16_true = torchvision.models.vgg16(pretrained=True)print(vgg16_true)train_data = torchvision.datasets.CIFAR10(&quot;dataset&quot;, train=True, transform=torchvision.transforms.ToTensor(),download=True)vgg16_true.classifier.add_module(&#x27;add_linear&#x27;, nn.Linear(1000, 10)) # 添加 层print(vgg16_true)print(vgg16_false)vgg16_false.classifier[6] = nn.Linear(4096,...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/image/icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">eric_zht</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%8D%B7%E7%A7%AF"><span class="toc-number">1.</span> <span class="toc-text">一、卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A1%AB%E5%85%85"><span class="toc-number">1.1.</span> <span class="toc-text">填充</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A5%E5%B9%85"><span class="toc-number">1.2.</span> <span class="toc-text">步幅</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98"><span class="toc-number">1.3.</span> <span class="toc-text">问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="toc-number">1.3.1.</span> <span class="toc-text">多输入通道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%BE%97%E5%88%B0%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93%EF%BC%9F"><span class="toc-number">1.3.2.</span> <span class="toc-text">如何得到多输出通道？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1x1-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">1.4.</span> <span class="toc-text">1x1 卷积层</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%9C%80%E5%A4%A7-%E5%B9%B3%E5%9D%87%E6%B1%A0%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">二、最大&#x2F;平均池化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81LeNet"><span class="toc-number">3.</span> <span class="toc-text">三、LeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E6%A3%80%E9%AA%8C%E5%B1%82%E7%9A%84%E5%B0%BA%E5%AF%B8%E6%9C%89%E6%B2%A1%E6%9C%89%E6%90%AD%E9%94%99%EF%BC%9A"><span class="toc-number">3.1.</span> <span class="toc-text">如何检验层的尺寸有没有搭错：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81AlexNet"><span class="toc-number">4.</span> <span class="toc-text">四、AlexNet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81VGG%E5%9D%97"><span class="toc-number">5.</span> <span class="toc-text">五、VGG块</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81NiN"><span class="toc-number">6.</span> <span class="toc-text">六、NiN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#NiN%E5%9D%97"><span class="toc-number">6.1.</span> <span class="toc-text">NiN块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NiN%E6%9E%B6%E6%9E%84"><span class="toc-number">6.2.</span> <span class="toc-text">NiN架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">6.3.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96-Batch-Normalization"><span class="toc-number">7.</span> <span class="toc-text">七、批量归一化 Batch Normalization</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E2%80%94%E2%80%94%E5%8A%A0%E9%80%9F%E6%94%B6%E6%95%9B%E3%80%81%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6"><span class="toc-number">8.</span> <span class="toc-text">——加速收敛、网络训练速度</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A"><span class="toc-number">8.1.</span> <span class="toc-text">解释</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E5%8C%85%E5%AE%9E%E7%8E%B0"><span class="toc-number">8.2.</span> <span class="toc-text">调包实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB%E3%80%81ResNet"><span class="toc-number">9.</span> <span class="toc-text">八、ResNet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E5%9D%97"><span class="toc-number">9.1.</span> <span class="toc-text">残差块</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B9%9D%E3%80%81%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">10.</span> <span class="toc-text">九、数据增强</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E5%A2%9E%E5%BC%BA%E6%96%B9%E6%B3%95"><span class="toc-number">10.1.</span> <span class="toc-text">常见增强方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BF%BB%E8%BD%AC"><span class="toc-number">10.1.1.</span> <span class="toc-text">翻转</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%87%E5%89%B2"><span class="toc-number">10.1.2.</span> <span class="toc-text">切割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%9C%E8%89%B2"><span class="toc-number">10.1.3.</span> <span class="toc-text">颜色</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">10.1.4.</span> <span class="toc-text">其他</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E3%80%81%E5%BE%AE%E8%B0%83"><span class="toc-number">11.</span> <span class="toc-text">十、微调</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E4%B8%AD%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">11.1.</span> <span class="toc-text">微调中的权重初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E9%87%8D%E7%94%A8%E5%88%86%E7%B1%BB%E5%99%A8%E6%9D%83%E9%87%8D"><span class="toc-number">11.2.</span> <span class="toc-text">1、重用分类器权重</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E5%9B%BA%E5%AE%9A%E4%B8%80%E4%BA%9B%E5%B1%82"><span class="toc-number">11.3.</span> <span class="toc-text">2、固定一些层</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E4%B8%80%E3%80%81%E9%94%9A%E6%A1%86"><span class="toc-number">12.</span> <span class="toc-text">十一、锚框</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#IoU-%E4%BA%A4%E5%B9%B6%E6%AF%94"><span class="toc-number">12.1.</span> <span class="toc-text">IoU 交并比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%8B%E4%BA%88%E9%94%9A%E6%A1%86%E6%A0%87%E5%8F%B7"><span class="toc-number">12.2.</span> <span class="toc-text">赋予锚框标号</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6%EF%BC%88NMS%EF%BC%89%E8%BE%93%E5%87%BA"><span class="toc-number">12.3.</span> <span class="toc-text">使用非极大值抑制（NMS）输出</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E4%BA%8C%E3%80%81%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95-R-CNN"><span class="toc-number">13.</span> <span class="toc-text">十二、物体检测算法 R-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B4%E8%B6%A3%E5%8C%BA%E5%9F%9F%EF%BC%88RoI%EF%BC%89%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">13.1.</span> <span class="toc-text">兴趣区域（RoI）池化层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Fast-RCNN"><span class="toc-number">13.2.</span> <span class="toc-text">Fast RCNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Faster-R-CNN"><span class="toc-number">13.3.</span> <span class="toc-text">Faster R-CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mask-R-CNN"><span class="toc-number">13.4.</span> <span class="toc-text">Mask R-CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">13.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E4%B8%89%E3%80%81%E5%8D%95%E5%8F%91%E5%A4%9A%E6%A1%86%E6%A3%80%E6%B5%8B-SSD"><span class="toc-number">14.</span> <span class="toc-text">十三、单发多框检测 SSD</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E9%94%9A%E6%A1%86"><span class="toc-number">14.1.</span> <span class="toc-text">生成锚框</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SSD%E6%A8%A1%E5%9E%8B"><span class="toc-number">14.2.</span> <span class="toc-text">SSD模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E5%9B%9B%E3%80%81YOLO-you-only-look-once"><span class="toc-number">15.</span> <span class="toc-text">十四、YOLO: you only look once</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E4%BA%94%E3%80%81%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2"><span class="toc-number">16.</span> <span class="toc-text">十五、语义分割</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%97%E4%B8%BERGB%E5%80%BC%E5%92%8C%E7%B1%BB%E5%90%8D"><span class="toc-number">16.1.</span> <span class="toc-text">列举RGB值和类名</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E4%BB%8ERGB%E5%88%B0VOC%E7%B1%BB%E5%88%AB%E7%B4%A2%E5%BC%95%E7%9A%84%E6%98%A0%E5%B0%84"><span class="toc-number">16.2.</span> <span class="toc-text">构建从RGB到VOC类别索引的映射</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E5%85%AD%E3%80%81%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="toc-number">17.</span> <span class="toc-text">十六、转置卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%E3%80%81%E6%AD%A5%E5%B9%85%E5%92%8C%E5%A4%9A%E9%80%9A%E9%81%93"><span class="toc-number">17.1.</span> <span class="toc-text">填充、步幅和多通道</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/25/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%202/" title="动手学深度学习 2">动手学深度学习 2</a><time datetime="2025-01-24T16:00:00.000Z" title="发表于 2025-01-25 00:00:00">2025-01-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/12/03/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%201/" title="动手学深度学习 1">动手学深度学习 1</a><time datetime="2024-12-02T16:00:00.000Z" title="发表于 2024-12-03 00:00:00">2024-12-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/27/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E4%B8%89%E5%A4%A9/" title="小土堆pytorch 第三天">小土堆pytorch 第三天</a><time datetime="2024-11-26T16:00:00.000Z" title="发表于 2024-11-27 00:00:00">2024-11-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/27/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E5%9B%9B%E5%A4%A9/" title="小土堆pytorch 第四天">小土堆pytorch 第四天</a><time datetime="2024-11-26T16:00:00.000Z" title="发表于 2024-11-27 00:00:00">2024-11-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/26/%E5%B0%8F%E5%9C%9F%E5%A0%86pytorch%20%E7%AC%AC%E4%BA%8C%E5%A4%A9/" title="小土堆pytorch 第二天">小土堆pytorch 第二天</a><time datetime="2024-11-25T16:00:00.000Z" title="发表于 2024-11-26 00:00:00">2024-11-26</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By eric_zht</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>